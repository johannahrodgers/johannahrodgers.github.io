
JUNE 9, 2025 DRAFT

Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969


1/ Continuity rather than discontinuity characterizes the progress of instrumental symbolic inscription practices and technical artifacts in the U.S. from 1865 to the present, and 

2/ These technical artifacts are shaped and informed by socio-cultural, political, and economic systems, which, In response to and as a result of these technical artifacts, change and alter in various ways, but that all of the time further consolidate the economic power of the owners of technical artifacts and thereby increase wealth inequality via the commodification of natural resources.   


Engineering Language uses verbal language as a case study to explore and instantiate the above hypotheses.


Engineering.Language.Harvard.University.Press.Proposal.November.9.2023.txt

Documenting and analyzing six key moments in the relationships amongst information and communications technologies defined broadly and definitions of verbal language, the project presents one reading of the "story" of how machines were taught to "read" and "write" in the U.S. from 1869 - 1969 by constructing one narrative through a collection of educational and technical documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” reappearing in 1969 as a machine readable character in the Unix operating system instructing a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers socio-cultural, technological, economic, political, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  It is a project that attempts to construct a history of instrumentalist inscription practices in nation building, public administration, education, information and communications technologies, and media networks. 

This project constructs one narrative through an archive of educational and technical documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” (re)appearing in 1969 as a human and machine readable character in the Unix operating system to instruct a digital electronic computing machine to write the contents of one file to another file location.

Documenting and analyzing six key moments in the relationships amongst information and communications technologies defined broadly and definitions of verbal language, the project presents one reading of the "story" of how machines were taught to read and write in the U.S. from 1869 - 1969.  I use the verb "taught" here in a non-metaphorical sense, because the manner in which non-humans (mechanical, electric, and digital electronic devices and networks) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  This educative process has also involved the re-definition of a number of distinctly human attributes, processes, and practices in order to accommodate non-human devices as, first, audiences for, and, later, as participants in, the production and exhange of human readable signs.  As a project about the histories of and relationships amongst instrumental symbolic inscription practices by humans and machines, it seeks to elucidate specific moments in the dialectical relationships between latin alphabetic and non-latin alphabetic systems of notation and between humans and machines and their mediation via higher education disciplines, military spending, literacy education practices, and the tools and technologies of representation, duplication, and inscription.

  

Through this documentary investigation, the project considers the many socio-cultural, medial, technological, economic, disciplinary, and educational policies, currents, and forces, and the dynamics of signification supporting each in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  

Seeking to better understand the many roles and functions of ICTs/technologies of inscription and their relationships with writing instruction in the pasts, presents, and futures of computing machines in the U.S., I plan to explore the historical connections between human and machine communication by analyzing six moments in the histories of human and machine communication and literacy "education."

Systems of shorthand, like other nineteenth-century innovations in the area of inscriptive practice and information and communications technologies (ICT), such as the telegraph, typewriter, phonograph, and telephone can be interpreted, as Lisa Gitelman has argued, as presenting their own unique “theories of language and textuality.”   

As literacy instruction, writing practices, and scholarly activities in the twenty first century U.S. become more and more intertwined with digital technologies and semiotics, the importance of understanding the genealogies of specific historical and contemporary ICTs, their intricate relations with the definitions and functions of human languages, and the cultural, technical, and medial contexts in which they emerged have become increasingly important in textual and writing studies (Fuller, Haas, Heim, Kirschenbaum). 

Engineering Language is a five act play with a nested structure:  
In the first act, speaking is re-defined as writing, i.e., an act of inscription and an unnatural attribute of humans.  
In the second act, inscription as an act in a human built world is re-defined as "inscription," or an anological act, i.e., action via analogy, and/or as a non-human act in a parallel electrical universe an electric writing system separate from physical reality [?]. (Is this AGB or Peano?)
In the third act, communication is redefined as product or event, not process; it is something that can be described by an equation representing the limits/calculus of abstract mechanical inscription.
In the fourth act, electro mechanical calculators are re-defined as “minds” that process "words".
In the fifth act, electro mechanical calculators are re-defined as "students" and programmed via scripts to become functionally “literate” devices.


This “play” begins and ends with shorthand “alphabets” as systems of notation that first enable humans to perform as instruments for other humans and that later enable machines to perform as instruments for humans who then instrumentalize other humans in different ways.  


This is a book about instrumental inscription practices by humans and machines and their relations.  It is a book about systems of shorthand and writing education, technical and educational standards, communications technologies, and military supported literacy and language research, their relations and the shifting boundaries between them in different medial environments/historical periods.  

Setting the Stage: The Bells and the Peirces at Harvard University

0.  Writing Studies and Systems of Notation: Reinscribing Inscription as an Instrument in a Semiotics of Human and Machine Communication   

1. Was AGB Raised as a Sparking Telegraph? Alexander Melville’s Visible Speech as Non-Latin Alphabbetic System of Notation, "Operating System" for Humans, and Educational "Standard"

The answers to this question depend first, of course, on your definitions of AGB and of the telephone. Both are complicated subjects. However, if you consider each as having functioned at various points in time as instruments for the production, replication, and transportation of graphic representations of human speech sounds, one answer to my apparently anachronistic question is "quite possibly."  

While the telephone may be an object more often conceived of as an instrument than young Scottish born men being trained to carry on the family business of elocutionary education, as it turns out, AGB was likewise employed for instrumentalist purposes.  Raised speaking several different dialects of English and Scottish, AGB was also a fluent speaker and writer in his father's non-latin alphabetic system of notation named "Visible Speech."  This chapter considers the long standing question of exactly what AGB "invented," by proposing that part of what he invented was "an analogy" of humans and machines as inscriptive devices. instrumental access to a non-human time/space continuum, and a new way of writing should perhaps be added to the list.  As a great deal of scholarship over the last fifty years has shown, the telephone was not invented by one individual.  However, the import of AGB's work persists in its lasting traces in institutional structures of the Harvard Composition Program and at Bell Technial Labs.     

2. Latin for the Masses: Mechanical Minds, Universal Languages and Markov’s Characters, or Symbolic Logic as Reading and Writing Instruments. Boole, Education, Peano, Phonetic Alphabet

3. Telemental Models of Communication: Hartley’s Equation and Saussure’s Wires

4. Before the Byte, There Was the Word: Von Neumann’s Word Choice

5. "The Computer Is an Extremely Fast Moron": Hopper’s Syllabus and Tukey’s Neologism

6. >, or How Processed Do You Like Your Humans: Thompson and Ritchie’s Unix Operating System as System of Notation and Electronic “Shorthand” 

Epilogue: Eliza Doolittle's Ghosts, or Mapping Relationships Amongst Hermeneutics and Semiology in the U.S.  // an Introduction to Hermeneutics and Semiology in Writing Studies in the U.S.: Exploring Relationships Between What and How Stories About Reading and Writing Are Told, or Inscriptive Media, Machine Learning, Translation, and Artificial Languages

The character who will eventually become Eliza Doolittle in the 1913 play “Pygmalion” by George Bernard Shaw and in the 1956 musical adaptation “My Fair Lady” and in the 1966 machine code of the computer program ELIZA began her life as a set of instructions for the production of human speech sounds in 1865.  Before she was ever a character, she was a set of graphic characters that Alexander Melville Bell, Alexander Graham Bell's father, fitted to the human speech organs to ensure their “proper” operations in the production of verbal language human speech sounds.  

Standard Written English is (re)inscribed into Eliza’s “body” in new ways at every stage of her existence, even in the times before she had a name and a biography.  

She is originally made up of "shorthand" characters from a non-latin-alphabetic system of notation for a proposed universal alphabet developed by Alexander Melville Bell. These characters are then used by AGB in his research into the transportation of sound over telegraphic networks and in his work with the deaf.  They are later incorporated into the international phonetic alphabet and, via the work of the linguist Henry Sweet, are conceived as the character of Eliza by George Bernard Shaw.  Disabilities, labor, class, public literacy education, race, commerce, nation building, and technologies of reproduction and communication are part of Eliza's history and inform every stage of her existence, even those that occurred before she was born.   

How the stories of Eliza as a set of "shorthand" characters, as a fictional character whose story is told via stage, print, and film, and, eventually, as machine code in a digital computational natural language processing application relate to and inform machine learning tools for the automated generation of human readable alphabetic text is the broad subject of this article.



Engineering.Language.Harvard.University.Press.Proposal.October.31.2023

Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969

Introduction

This project documents and analyzes the dialectical relationships between information and communications technologies defined broadly as including technologies of "print" and social "technologies" of education (Bazerman) and definitions of verbal language to offer one reading of the story of how machines were taught to read and write in the U.S. from 1869 - 1969.  I use the verb "taught" here in a non-metaphorical sense, because the manner in which non-humans (mechanical and digital electronic devices and networks) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  This educative process has also involved a great deal of re-definition of distinctly human attributes, processes, and practices in order to accommodate non-human devices as, first, audiences for, and later as participants in the production and exhange of human readable signs. 

It proposes that there is some relationship between the representation of humans and their affordances as mechanisms and the personification of machines as humans and that the education of machines and the education of humans in the U.S. may exist in a dialectical relationship.  The two are inextricably bound up with one another.  It is a complicated story about humans, government, and nation building.  Part of the nation's military budget was always being dedicated to literacy education.  It was used to teach machines to read and write instead of humans.  The reasons for this are diverse and probably involve the relations of social technologies with material technologies.  I have only scratched the surface in this project and by making the materials I have used for this project available to other researchers, it is my hope that more will be revealed.  This is the story of latin alphabetic systems of notation being made to control/merged with non-latin alphabetic systems of notation.     

Textbooks, linguistics, public literacy education policy and practices, government spending, mechanical representation, reproduction, and distribution technologies, writing machines, human sciences, definitions of the human, figurative language, definitions of language are involved.  

This is a project about instrumentality and definitions of language in relation to human and machine communications.  This is a project about communication and instrumentality.  This project documents and analyzes relationships amongst the definitions and affordances of human and machine communication practices from 1869 - 1969. 

We have undoubtedly reached a new point in human/machine relations when a comparative literature major can begin to think of the latin alphabet as but “one system of notation” among others.  

How do engineers think about language?  Do they?  I’ve asked myself this question for some time and my attempt to answer the question may be the best way to describe this book and its purposes.   

Applied Linguistics 
The Manual Humanities
Sociology of Literacy 

The potential impact of AI on literacy practices and literacy education, and the cognitive processes associated with them is one that; cognition and education are mediated by social and material technologies.  

The character who will eventually become Eliza Doolittle in the 1913 play “Pygmalion” by George Bernard Shaw and in the 1956 musical adaptation “My Fair Lady” and in the 1966 machine code of the computer program ELIZA began her life as a set of instructions for the production of human speech sounds in 1865.  Before she was ever a character, she was a set of graphic marks that Alexander Melville Bell fitted to the human speech organs to ensure their “proper” operations.  Standard Written English is written into Eliza’s “body” in new ways at every stage of her existence, even in the times before she had a name and a biography.  She is originally made up of shorthand characters from a non-latin-alphabetic system of notation for a proposed universal alphabet.  Both those characters and eventually Eliza's character are given life by being applied [through their embodiment via] to the human speech organs.  Disabilities, labor, class, education, race, commerce, nation building, and technologies of reproduction are part of Eliza's history and inform every stage of her existence, even those that occurred before she was born. 

How the stories of Eliza as a set of "shorthand" characters, as a fictional character whose story is told via stage, print, and film, and, eventually, as machine code for digital computational natural language processing relate to and inform ChatGPT and other generative AI tools for the automatic composition of human readable alphabetic text is the broad subject of this book.  

By investigating the history of relationships between instrumentality and public literacy education [verbal language] in the United States from the post-Civil War period to the post-World War II period and the dialectical relationshihps between humans and instruments mediated by verbal language in the same period, I document the ways in which instrumental uses of verbal language by humans has been instrumental in endowing electro-mechanical devices with the ability to manipulate human readable signs in a human like manner.  The reasons for the extended time period under investigation relate to the interconnections of 19th and 20th c. information and communications technologies and of the commercial and educational networks from the same period. 

This is a project about instrumentality and literacy.  It is also a book about humans and machines and their relations as mediated by symbolic systems used to encode verbal language.  

---

This is a project informed by media archaeology and forensic bibliography in their most literal definitions.  
It constructs one narrative through an archive of documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” reappearing in 1969 as a machine readable character in the Unix operating system instructing a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, political, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  

It is a project that seeks to construct a history of instrumentalist inscription practices in relation to nation building, public administration, education, information and communications technologies (commerce?), and media networks.  

This project presents one explanatory narrative path through a collection of archival documents to consider what processes are involved in verbal language becoming a domain of electronic digital computation. 

One approach to this question is by thinking of verbal language as a user interface with an implied "human" as adjective placed in front of "user."  There has been and continues to be some excellent work being done in this area.  What distinguishes this project from those projects and from many other cultural histories of computation and information and communications technologies that have informed it is its attention to the roles of instruments as users in processes of verbal signification and their construction as symbolic systems over a one hundred year period. 

          
---

What connections might exist between the reclining caret in the Unix operating system command line and a printed character signifying the emission of breath accompanying the formation of a human speech sound in a nineteenth century printed manual of "shorthand"? How might an exploration of these connections help us to better understand the many roles and functions of technologies of inscription and their relations with writing and language education in the pasts, presents, and futures of computing machines?  While these may sound like intriguing if somewhat implausible hypotheses, this project seeks to explain how these two apparently separate realms of communication--human/machine; print/digital--are related to one another and the disciplinary, technical, and socio-economic, and cultural histories mediating their relationships.  

the connections between human and machine communication can be documented.  The story I plan to tell connects these two apparently disparate realms of communication (human/machine; print/digital) by tracing the remediation of spoken communication in Alexander Melville Bell’s system of "shorthand," which he called Visible Speech, and its afterlives as a series of material, medial, and socio-cultural artifacts result of its influence in the technical, academic, and cultural imagination. 

What connections might exist between a symbol signifying the emission of breath used in the production of a spoken word in a printed diagram from an 1867 manual of shorthand by Herman Melville Bell and an identical symbol in the Unix operating system command line?  Both are  characters in a non-latin alphabetic systems of notation for the description and execution of an act of inscription.  That the Unix shorthand character instructs a digital electronic machine to act and the Visible Speech shorthand character instructs a human to act make their connections all the more intriguing.   

Could these graphical marks, despite their medial and temporal separation, be in some way related? 
 Graphically, there is no doubt that they can be connected: the greater than sign ( > ), as it is familiarly called, is a very common typographical mark used in any number of print and digital applications.  
But what about the provenance of these two specific tokens of this particular type?  As it turns out, they both act as instructions for acts of "communication" and both originate from work associated with Bell Technical Labs.  
What is more, both marks are instructions for an act of inscription.  That the audience for the instruction in the Unix operating system is a machine while the audience for the character in Melville Bell's Visible Speech is humans whose speech organs are being represented as a mechanical system makes their connections all the more intriguing.    if we define Unix as a kind of “digital shorthand,” as the New York Times did in explaining its operations to a lay audience in a 2011 obituary of Dennis Ritchie, one of the authors of the Unix operating system, the possible connections between these two marks become all that more intriguing (Lohr, NYTimes, October 11, 2011).  

Seeking to better understand the many roles and functions of technologies of inscription and writing instruction in the pasts, presents, and futures of computing machines, I plan to explore the historical connections between human and machine communication by analyzing the conditions that gave rise to Herman Melville Bell’s system of shorthand, which he called “Visible Speech,” its roles in the development of new communications technologies in the mid- and late- nineteenth century U.S., and its afterlives as a result of its influence and remediation in the technical, academic, and cultural imagination. 

Systems of shorthand, like other nineteenth-century innovations in the area of inscriptive practice and information and communications technologies (ICT), such as the telegraph, typewriter, phonograph, and telephone can each be interpreted, as Lisa Gitelman has argued, as presenting their own unique “theories of language and textuality.”   

As composition instruction, writing practices, and scholarly activities in the twenty first century U.S. become more and more intertwined with digital technologies, the importance of understanding the genealogies of specific historical and contemporary ICTs, their intricate relations with the definitions and functions of verbal language, and the cultural, technical, and medial contexts in which they emerged have become increasingly important in the fields of the digital humanities and writing studies (Fuller, Haas, Heim, Kirschenbaum). 


Context


What the Artifacts Assume About Human Literacy


Findings



Engineering Language is a five act play with a nested structure:  
In the first act, speaking is re-defined as inscription.  
In the second act, inscription is re-defined as a writing system divorced from reality [?]. (Is this AGB or Peano?)
In the third act, communication is redefined as product or event, not process; it is something that can be described by an equation representing the limits/calculus of abstract mechanical inscription.
In the fourth act, electro mechanical calculators are re-defined as “minds”.
In the fifth act, electro mechanical calculators are re-defined as functionally “literate” devices.

This “play” begins and ends with shorthand “alphabets” as systems of notation that first enable humans to perform as instruments for other humans and that later enable machines to perform as instruments for humans formerly performing as instruments.  


This is a book about instrumental inscription practices by humans and machines and their relations.  It is a book about shorthand and longhand, their relations and the shifting boundaries between them in different medial environments/historical periods.  

Genre literature and the manual humanities (see Jennifer Wick 1992 article in Victorian Studies)


This is a book of forensic bibliography (Greetham) focused on a set of educational, technical and government documents for the purpose of better understanding the socio-cultural, technological, and economic currents shaping human and machine instrumental inscripton practices and their relationships.  It constructs one narrative through an archival print archive spanning approximately one hundred years to explain how a mark from a book printed in 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” (re)appearing in 1969 as a machine readable character in the Unix operating system depicting an instruction for a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary archival investigation, the book considers how socio-cultural, technological, economic, political, and educational policies, currents, and issues are involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.   

The book is written for an interdisciplinary audience.  The purpose of the book is two fold: first, it seeks make visible the dialectical relationships between human and mechanical literacies for those working in education and the humanities and, secondly, to establish a framework and digital archive for further study of a history of instrumentalist inscription practices in nation building, public administration, education, information and communications technologies, and media networks in the United States from the end of the Civil War through the post-World World II period. 
 
The period and topics addressed in this book have been studied extensively.  The books that are most comparable to this book are David Mindell's 2002 _Between Human and Machine: Feedback, Control, and
Computing before Cybernetics_ and Matthew Kirschenbaum's ___ Mechanisms.  The work of Lorraine Daston, Lisa Gitleman, and Johanna Drucker have also been formative.  This book has been influenced by Gitleman's Scripts, Grooves, and Writing Machines, Digital Energetics, in This book distills and references dozens of cultural and historical studies of media and technologies in the history of the United States.   What distinguishes this project from others in the histories of media and science and technology studies is the dialogue established between these two fields and the history of literacy education in the United States.  Proposing that the literacy education of mechanical and mechanical-electronic devices exists in a dialectical relationship to human literacy education in the United States, the book seeks to map the intersections of human and “machine” education with a particular focus on the use, application, and management of human verbal language in the process.   
The book is a timely one for many reasons, one of the most obvious being its relations to the technologies and applications of web-based generative large language models as information and communications tools and applications. As “logical” extensions of text editing and processing applications combined with a deluge of virtually real time multimedia input (including alphabetic) and machine “learning” tools, generative LLMs like GPT3, Bard, ____, are the public manifestation of a fact that has been apparent for over two decades: Verbal language is a domain of computation and it has been since humans started communicating with machines.  

When I began my work on this research in 2014, I did so in order to understand the question of “How verbal language became a computational 'problem.'”   Somewhat to my surprise, though there was evidence all around me of how verbal language had become a computational “problem,” developing a plausible response to this question was complicated by numerous disciplinary and technical factors.  



Writing is a medium (how/process)
Writing is a conversation (why/rhetoric)
Writing is a symbolic system/currency (what/genre)

April 27, 2023

Instrumentality, Communication, and Systems of Notation

What are the affordances of the machine?

How is communication defined?

How is language defined?

How is verbal language defined?

How is information defined?

Moro_Syntax_and_the_brain.pdf




Ideas from The Prague School of functional linguistics have been adopted in NLP

Roman Jakobson writing about bits in his 1961 essay “Linguistics and Communication Theory” (theme/rheme emerges from the Prague School)

In this 1954 review of Willis’ book, the author proposes somewhat wistfully that the “inorganic world of mechanics” and the “immaterial realm of thought” might be brought together via a shared theoretical approach such as entropy and thermodynamics “Engineering.Language.Navy.Research.Lab.Review.Jackson.Communication.Theory.1953.1 

ChatGPT is one version of a mind, not a mind per se.  It is the “literate mind” that is discussed and described in the abstract and that has now been built thanks to a great deal of money and education.  

Project Overview

This project documents a genealogy of the relationships between human and non-human language and literacy education--and their relationships--from 1869 - 1969.   information and communications technologies defined broadly [as including technologies of print reproduction and social technologies (see Bazerman) and their relationships with definitions of language and practices of language education in the U.S. from 1869 - 1969.  Textbooks, linguistics, public literacy education policy and methods, government spending, mechanical representation, reproduction, and distribution technologies, writing machines, human sciences, definitions of the human, figurative language, definitions of language.  This is one version of the story of how machines were taught to read and write in the U.S.  I use the verb "taught" in a non-metaphorical sense, because the manner in which non-humans (mechanical and digital devices) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  The education of machines and the education of humans in the U.S. exists in a dialectical relationship.  The two are inextricably bound up with one another.  It is a complicated story about humans, government, and nation building.  Part of the nation's military budget was always being dedicated to literacy education.  It was used to teach machines to read and write instead of humans.  The reasons for this are diverse and probably involve the relations of social technologies with material technologies.  I have only scratched the surface in this project and by making the materials I have used for this project available to other researchers, it is my hope that more will be revealed.  This is the story of latin alphabetic systems of notation being made to control/merged with non-latin alphabetic systems of notation. 

ChatGPT is the latest instantiation of a complex of non-latin alphabetic systems of notation controlling the production and manipulation of latin alphabetic tokens, which humans usually refer to as words.  

medium = thing

mode (is this term always already a machine term?)

sense

cognition

The model used to teach digital electronic calculating machines to write was the current-traditional model of SWE education.  


Introduction

[What if] How might the reclining caret in the Unix command line can be in some way be connected to a mark signifying the emission of breath accompanying the formation of a spoken word?  What might that tell us about some of the many roles and functions of writing in the pasts, presents, and futures of computing machines?  While this may sound like an intriguing if somewhat implausible hypothesis, I hope to show that this connection between human and machine communication can be documented.  The story I would plan to tell provides one narrative connecting these two apparently disparate realms of communication (the human and machine) and in the process also traces connections amongst the spoken and the written, and the manual, mechanical, and digital, in ways that may offer some insights into the relationships amongst old and new technologies of inscription and representation and the social and material technological systems supporting them.  

First, there is the questions of what issues need to be addressed in establishing a framework between human inscription and representation practices and mechanical inscription and representation "practices."  


Although the book is busy enough with the histories of verbal signification and computation to not have much time for a consideration of the genealogies of “Artificial Intelligence,” it is implicitly connected to those technologies and narratives through its concern with alphabetic and machine literacies.  As Stephanie Dick has written in her work on the histories of Artificial Intelligence, “making up minds,” is, from a technical and cultural perspective, very much the concern of current media representations and technical implementations of what is now referred to generally as “AI.”   

	Introduction: Hermeneutics and Semiology: Exploring Relationships Between What and How Stories About Reading and Writing Are Told


Was AGB Raised as a Telephone? Alexander Melville’s Visible Speech as an Operational System of Notation and Educational Standard:


Latin for the Masses: Universal Languages and Markov’s Characters, or Logic and Statistics as Reading and Writing Instruments:


Telemental Models of Communication: Hartley’s Equation and Saussure’s Wires:


Before the Byte, There Was the Word: Von Neumann’s Word Choice:


Hopper’s Syllabus and Tukey’s Neologism
>, or How Processed Do You Like Your Humans: Thompson and Ritchie’s “Shorthand” for the Unix Operating System

	Epilogue: Eliza Doolittle's Ghosts

There is a lot that is not in this book that it is my hope that other scholars and historians will further investigate these themes and topics using the digital archive and bibliography accompanying this project.  

This project is imbricated with my own literacy narrative, which began in the American midwest in 1968, was accredited first at Stanford University in 1990 when I graduated as a Comparative Literature major, developed at Yale University as a graduate student and in an applied professional capacity in Silicon Valley after my graduation from college, and broadly expanded by my work as a professional writer in New York City and my research and teaching with students at The City University of New York.  I do not hesitate these days to say that I have made my living as a professional in high tech, first in the private sector and later in the public sector.  High tech has become a part of every industry in the last thirty years, so this should not come as any great surprise.  However, what I did not understand until becoming more involved with the histories of computing(s) was how much my educational training in philosophy, linguistics, writing, and textual studies had to do with engineering and its various disciplines.    

 
The problems this project seeks to address include:

1/ How digital electronic networked computational devices learned how to read and write, the discreet stages of this process, and the relationships between the technical engineering of material artifacts and of socio-cultural systems at each stage. 

2/ Why and how Standard Written English as a dialect becomes embedded in digital electronic networked computational devices and the socio-cultural and educational import of this. 

3//How rhetoric functions in technical documents to negotiate the boundaries between human and machine instrumentality and to promote the interests of some parties and interests over others.   

To address these issues, several popular and disciplinary myths need to be addressed: 

1/ That digital electronic computing systems have always been symbolic systems that “educated” themselves through automated and systematic ingenuity.
  
2/ That humanist analyses and critiques of human languages and its socio-cultural functions are of no import to the technical or applied applications of human and machine languages.   

3/ That the simultaneous deployment of  technical and instrumental uses of verbal language is not an event of some significance in the histories of computing and the histories of education in the U.S.. 

4/ That the expenditures on the higher literacy education of machines has been both inevitable and had anything but beneficial effects on the processes and purposes of human literacy education.   

* What previously unknown or unfortunately neglected story are you planning to tell?
How machines learned to read and write in the U.S. from 1869 - 1969 and how this “literacy narrative” is interwoven with the socio-economic, medial, technical, cultural, political, and educational currents of the same period.  Comparing and contrasting the rise of mechanical literacy and human literacy.  Inverse relationship.  

This book is about systems of notation.

* How is this book different from all other books?

It defines its difference from other books as provisional and invites scholarly collaboration in the discussion and research.  It presents its argument as one interpretation of a set of documentary evidence published in a shared digital archive for other scholars to use and expand upon.

It provides a rational for the field of the Manual/Applied Humanities/Critical Bibliography and Textual Studies by considering a history of instrumental inscriptive practices as including both social technologies (public literacy education, government funded initiatives related to defense, commerce, education, and administration, definitions and implementations of language in a range of higher educational disciplines) and material technologies (shorthand “alphabets,” printing presses, telegraphs, telephones, typewriters, teletypewriters, adding machines, cash registers, vacuum tubes, machine languages, keyboards, memory units, semiconductor chips, software systems and applications)  and traces the meeting of these two technological domains at specific moments in time based on evidence from discursive artifacts in government and corporate archives and publications.

It is written for a general audience.  It synthesizes the arguments of forty years of socio-cultural studies on computation and its relationships with literacy education practices and policies for a general audience.  It is an interdisciplinary documentary project aimed at synthesizing existing historical and cultural work on the histories of computation.  

It is relevant to contemporary issues and events. It suggests that computational technologies and their relationships with the socio-cultural, environmental, educational, and economic interests of the public need to be managed in the interests of something other than the logic of technics (Winner) as it has come to be defined in the early twenty-first century, i.e., efficiency and cost savings, to protect the well being of biological organisms, including humans.    

It is collaborative and provisional. 

Import

Audience


* Why does that matter? To whom?
Possible audiences are as variable as publishers. Consider:
* Is your book for specialists in your field?  It is for scholars working in the interdisciplinary fields of media archaeology, the manual humanities, software studies, science and technology studies
* Does your book focus on a particular area within a larger field?  
* Is it a book that students might use, and if so, students at what level?  It would be useful to graduate students in education, writing studies, STS.
* Is it a “trade” book? That is, one intended for general readers, those without specialized knowledge in your area?
Whatever your answer, consider carefully the kind of approach, terminology, level of explanation, and scholarly apparatus that your book will need to make it most compelling for your ideal reader.
Successful proposals usually include:
* A narrative description of the proposed book’s themes, arguments, goals, place in the literature, and expected audience. State your argument concisely and clearly.

How, when, and why did digital electronic networked computational devices become symbolic processing machines and how, when, and why do assumptions about and practices related to symbolic “processing” by humans inform the “literacy narratives” of these mechanical and electronic calculators and communications devices?  There are  several complications in posing this question in this form.  The first set of complications arises with the determined persistence with which  the myth that electronic computational devices sometime around 1950 simply arose as symbol processing machines (Dreyfus and Dreyfus 1988).  This is an ahistorical depiction of the emergence of these machines that has been reinforced by media networks and corporate interests.  These devices and the social and material technologies embedded in them have histories that have been well documented by scholars in a range of fields.  Yet, the dominant media and commercial narratives of symbolic processing and its applications, what is now generally categorized under the name of artificial intelligence, is ahistorical.   

The second set of complications arise in relation to the breadth and depth of the disciplinary specialization and technical expertise required to respond to a question involving the histories of so many different disciplines: computational devices, mathematics, psychology, brain science, linguistics, and engineering.  Yet, within engineering, applied versions of each of these disciplines are used in research and development of products and technologies and in 1986, this interdisciplinary field was given a name and an institutional role at Stanford University with the creation of the Symbolic Systems program.  While it is not possible to publish a scholarly book that encompasses the histories of the disciplines of linguistics, psychology, cognitive science, neurosurgery, and public education, applied versions of each of these disciplines are borrowed and freely adapted in the development of products and new technologies related to human language processing.  While there is an interdisciplinary field called “Symbolic Systems,” its history has been written in any number of disciplines making up the field.  This project suggests that this field of Symbolic Systems requires a history that includes a consideration of the material and the social technologies of signification and symbol making via archival textual evidence.      
The histories of public literacy education and the histories of machine “literacy” in the United States share an identical timeline.  Yet the two are rarely/surprisingly brought into dialogue with one another in relation to their shared concern with social and material technologies of inscription, or what is sometimes referred to as “writing.”  The term writing is problematic because it signifies so many different things depending on the context in which it is deployed.  It can signify making a marks on a page, the graphic depiction of speech, _____, or ______.   This term “writing,” whether being used as a synonym or synechdoche for literacy is one unique both in its vagueness/generality and scope/breadth.  Particularly in a socio-educational context this term “writing” as it relates to advanced literacy in Standard Written English is integral to a range of educational, social, cultural, and commercial operations and functions.  Instrumental uses of social and material technologies of inscription.  
                     
Having dedicated most of my education and professional life to understanding how humans read and write and the import of both acts, I also, because of the economic realities of my life and job market, spent ten years of my professional life becoming conversant in how electronic digital computers operate.  The interconnections and relationships between the literacy practices of both humans and digital electronic calculators have been addressed by scholars in the social sciences and STS in different disciplines but never from the interdisciplinary perspective of instrumental literacy and signification functions.  
The scholarship on the roles of technologies in K-12 and higher ed writing instruction are distinct.  Until the early 2000s, there was surprisingly little scholarship in higher ed writing studies dedicated to discussions of writing technologies.  Christina Haas Writing Technology: Studies on the Materiality of Literacy (1996) is one exception.  Charles Bazerman’s Languages of Edison’s Light (1999) may be another though this book is usually categorized as being part of the history of science and technology studies rather than as part of the history of composition.  


Lorraine Daston: a history of inscriptive practices and technologies in Nation Building: Writing Studies, Literacy Education, Media Archaeology.  The Manual Humanities, which prioritizes the needs and specificities of human cognitive processes, human labor practices, and humans as a species in a highly delicate and interconnected global physical, social, biological, and geological environment.              
* A comparison of the proposed book to other books now available that are intended for the audience you seek. (If you are writing a specialized monograph, it is not especially illuminating to compare it to a popularized treatment of the same subject.)
David Golumbia
Matthew Fuller
Wendy Chung
Anne Pasek, et al
Cubitt, Sean. 2017. Finite Media: Environmental Implications of Digital Technologies. Durham, N.C.: Duke University Press.
* A summary of your own professional experience, past publications, and relevant research, aimed at explaining why you are the right author for the book you intend to write.

My own literacy, educational, and professional narrative exists in a metonymic relationship with the story told in this book. Remediation, disciplinary agonism, and the roles of capital in higher education is part of what makes me uniquely qualified to write this book.  [Few people other than me could write this book because the story is in part a refraction of my own literacy and educational narrative.]  Born in 1968, I was raised and educated in Ann Arbor, Michigan, where I attended public school.  First, Burns Park Elementary School, which was one of the first public elementary schools built by the newly formed Department of Education in the U.S. Later, Tappan Middle School, named after _____, and finally Pioneer High School. 


Even in the early 1970s cultures of print were already facing off with new digital electronic technologies, one material trace of which was the x by y punch cards inside the front cover of every book sold at the local university bookstore, Borders.  At that time, one of the two Borders brothers was enrolled at the University of Michigan and had access to the mainframe computer.  He used the computing time granted to him to write a program for the inventory management of the bookstore.  Each night, ____ would collect the data cards from that day’s sales and update the store’s inventory.  I have only third hand information about the specific programs used, but I know that this early (1972) management and integration of sales and inventory systems were in part what led to the success of Borders Bookstore both as an independent business and later as a chain that was acquired by KMart.
I graduated from high school in 1986 and attended Stanford University as an undergraduate on a full scholarship, majoring in Comparative Literature.  Why I did not study computer science is, in the 21st century, hard for me to comprehend.   However, at the time, this decision had to do with how the subject was taught –and largely still is taught– that I was categorized as a “verbal,” not a “math” person, and that the humanities, particularly textual studies, still appeared to be highly relevant to society and culture.  

Having been provisionally hired by the first management consulting firm in Silicon Valley in 1992 after I took a leave of absence from a Ph.D. Program in Comparative Literature at Yale University and was unable to find a job in print publishing in the San Francisco Bay Area, I spent the next ten years researching new technologies, primarily database software, in an office park shared by Sun Microsystems.  At that time, I never once considered learning Unix or SqlServer even though I discussed that operating system and database scripting language daily with the IT managers whom I interviewed and the marketing, communications, and strategic development officers at the then F1000 technology companies who were our clients.  I was privy, in 1995, to early discussions about the launch of Adobe’s Portable Document Format standard, which I would only come to understand in the 2020s as the corporate standardization of a publicly available file standard.  My colleagues at the management consulting firm, most of whom had attended Yale rather than Stanford, and who therefore knew what it actually meant to be a “Comparative Literature major” were amused, astonished, and pleased at how well a degree in philosophy, linguistics, and textual studies had prepared me to work as a research analyst in the high tech industry.    

In 1995, I relocated to Brooklyn, New York, continued to work remotely for the management consulting firm in Silicon Valley, and enrolled in a graduate creative writing program at the City College of New York.  It was by completing this program that I first became an adjunct writing instructor at the City University of New York, where I would complete my Ph.D. in English/Composition and Rhetoric in 2017.  I first enrolled at CCNY because I aspired to have a career in creative writing and I chose to attend a degree program I could easily afford.  The tuition at CUNY has since 1995 quadrupled.  My experiences in public higher ed have been, both as a student and as an instructor, decidedly mixed.  I believe in the mission of public education and yet I was also aware that I was treated much better at a private college.      

My intellectual, creative, and professional lives have been shaped by the intersection of the medial, technical, and economic realities in which I was raised and in which I have lived as part of a specific generation of well educated humans in the United States.  I would like humans who do not currently work in Engineering  and the disciplines related to it to know, first, that human readable latin alphabetic language, its uses and definitions, has had a lot to do with how and why decisions have been made to prioritize the interests of electronic computational devices and the systems supporting them over those of socio-cultural and planetary conservation, and, second, that, regardless of their disciplinary background and level of education,  they are qualified and capable of engaging in conversations related to decisions about the priorities for public policy and public spending that will be made in the next ten to twenty years.          

* An annotated table of contents, with a brief description of the contents of each chapter.
* An estimate of the probable length of the book, the illustrations (if any) that you wish to include, the time it will take you to write it, and any possible complicating factors.

Please send all proposals by email to submissions_HUP@harvard.edu (Attention: Editorial). You should expect an answer within three to four weeks. If you choose to send your proposal by U.S. mail or courier please be advised that we are not responsible for lost packages and we do not return unsolicited manuscripts or proposals.

Writing Studies

Linguistics

Semiotics

STS/Computer History

STS/Inscriptive Practices: Donna Haraway, Lily Kay, Friedrich Kittler, Lisa Gitelman, Johanna Drucker, Stephanie Dick, Chung, what's his face, Charles Bazerman, Sean Cubitt, 

Media Studies and Archaeology

Bibliography

Composition and Rhetoric/Communication Studies

Digital Humanities

SEPTEMBER 14 2023 DRAFT

Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969


1/ Continuity rather than discontinuity characterizes the progress of artifactual electro-mechanical technologies in the U.S. from 1865 to the present, and 


2/ These artifactual technologies are shaped and informed by socio-cultural technologies, which, In response to and as a result of these artifactual technologies, change and alter in various ways, but that all of the time further consolidate the economic power of the owners of artifactual technologies and thereby increase wealth inequality via the commodification of natural resources.   


Engineering Language uses verbal language as a case study to explore and instantiate the above hypotheses.







Engineering.Language.Harvard.University.Press.Proposal.November.9.2023.txt

Documenting and analyzing six key moments in the relationships amongst information and communications technologies defined broadly and definitions of verbal language, the project presents one reading of the "story" of how machines were taught to "read" and "write" in the U.S. from 1869 - 1969 by constructing one narrative through collection of educational and technical documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” reappearing in 1969 as a machine readable character in the Unix operating system instructing a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, political, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  It is a project that attempts to construct a history of instrumentalist inscription practices in nation building, public administration, education, information and communications technologies, and media networks. 

This project constructs one narrative through an archive of educational and technical documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” (re)appearing in 1969 as a human and machine readable character in the Unix operating system to instruct a digital electronic computing machine to write the contents of one file to another file location.

Documenting and analyzing six key moments in the relationships amongst information and communications technologies defined broadly and definitions of verbal language, the project presents one reading of the "story" of how machines were taught to read and write in the U.S. from 1869 - 1969.  I use the verb "taught" here in a non-metaphorical sense, because the manner in which non-humans (mechanical, electric, and digital electronic devices and networks) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  This educative process has also involved the re-definition of a number of distinctly human attributes, processes, and practices in order to accommodate non-human devices as, first, audiences for, and, later, as participants in, the production and exhange of human readable signs.  As a project about the histories of and relationships amongst instrumental symbolic inscription practices by humans and machines, it seeks to elucidate specific moments in the dialectical relationships between latin alphabetic and non-latin alphabetic systems of notation and between humans and machines and their mediation via higher education disciplines, military spending, literacy education practices, and the tools and technologies of representation, duplication, and inscription.

  

Through this documentary investigation, the project considers the many socio-cultural, medial, technological, economic, disciplinary, and educational policies, currents, and forces, and the dynamics of signification supporting each in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  





 



Seeking to better understand the many roles and functions of ICTs/technologies of inscription and their relationships with writing instruction in the pasts, presents, and futures of computing machines in the U.S., I plan to explore the historical connections between human and machine communication by analyzing six moments in the histories of human and machine communication and literacy "education."

Systems of shorthand, like other nineteenth-century innovations in the area of inscriptive practice and information and communications technologies (ICT), such as the telegraph, typewriter, phonograph, and telephone can be interpreted, as Lisa Gitelman has argued, as presenting their own unique “theories of language and textuality.”   

As literacy instruction, writing practices, and scholarly activities in the twenty first century U.S. become more and more intertwined with digital technologies and semiotics, the importance of understanding the genealogies of specific historical and contemporary ICTs, their intricate relations with the definitions and functions of human languages, and the cultural, technical, and medial contexts in which they emerged have become increasingly important in textual and writing studies (Fuller, Haas, Heim, Kirschenbaum). 

Engineering Language is a five act play with a nested structure:  
In the first act, speaking is re-defined as writing, i.e., an act of inscription and an unnatural attribute of humans.  
In the second act, inscription as an act in a human built world is re-defined as "inscription," or an anological act, i.e., action via analogy, and/or as a non-human act in a parallel electrical universe an electric writing system separate from physical reality [?]. (Is this AGB or Peano?)
In the third act, communication is redefined as product or event, not process; it is something that can be described by an equation representing the limits/calculus of abstract mechanical inscription.
In the fourth act, electro mechanical calculators are re-defined as “minds” that process "words".
In the fifth act, electro mechanical calculators are re-defined as "students" and programmed via scripts to become functionally “literate” devices.


This “play” begins and ends with shorthand “alphabets” as systems of notation that first enable humans to perform as instruments for other humans and that later enable machines to perform as instruments for humans who then instrumentalize other humans in different ways.  


This is a book about instrumental inscription practices by humans and machines and their relations.  It is a book about systems of shorthand and writing education, technical and educational standards, communications technologies, and military supported literacy and language research, their relations and the shifting boundaries between them in different medial environments/historical periods.  

0.  Writing Studies and Systems of Notation: Reinscribing Inscription as an Instrument in a Semiotics of Human and Machine Communication   

1. Was AGB Raised as a Telephone? Alexander Melville’s Visible Speech as, System of Notation, "Operating System" for Humans, and Educational "Standard"

The answers to this question depend first, of course, on your definitions of AGB and of the telephone. Both are complicated subjects. However, if you consider each as having functioned at various points in time as instruments for the production, replication, and transportation of graphic representations of human speech sounds, one answer to my apparently anachronistic question is "quite possibly."  

While the telephone may be an object more often conceived of as an instrument than young Scottish born men being trained to carry on the family business of elocutionary education, as it turns out, AGB was likewise employed for instrumentalist purposes.  Raised speaking several different dialects of English and Scottish, AGB was also a fluent speaker and writer in his father's non-latin alphabetic system of notation named "Visible Speech."  This chapter considers the long standing question of exactly what AGB "invented," by proposing that part of what he invented was "an analogy" of humans and machines as inscriptive devices. instrumental access to a non-human time/space continuum, and a new way of writing should perhaps be added to the list.  As a great deal of scholarship over the last fifty years has shown, the telephone was not invented by one individual.  However, the import of AGB's work persists in its lasting traces in institutional structures of the Harvard Composition Program and at Bell Technial Labs.     

2. Latin for the Masses: Mechanical Minds, Universal Languages and Markov’s Characters, or Logic and Statistics as Reading and Writing Instruments. Boole, Education, Peano, Phonetic Alphabet

3. Telemental Models of Communication: Hartley’s Equation and Saussure’s Wires

4. Before the Byte, There Was the Word: Von Neumann’s Word Choice

5. "The Computer Is an Extremely Fast Moron": Hopper’s Syllabus and Tukey’s Neologism

6. >, or How Processed Do You Like Your Humans: Thompson and Ritchie’s Unix Operating System as System of Notation and Electronic “Shorthand” 

Epilogue: Eliza Doolittle's Ghosts, or Mapping Relationships Amongst Hermeneutics and Semiology in the U.S.  // an Introduction to Hermeneutics and Semiology in Writing Studies in the U.S.: Exploring Relationships Between What and How Stories About Reading and Writing Are Told, or Inscriptive Media, Machine Learning, Translation, and Artificial Languages

The character who will eventually become Eliza Doolittle in the 1913 play “Pygmalion” by George Bernard Shaw and in the 1956 musical adaptation “My Fair Lady” and in the 1966 machine code of the computer program ELIZA began her life as a set of instructions for the production of human speech sounds in 1865.  Before she was ever a character, she was a set of graphic characters that Alexander Melville Bell, Alexander Graham Bell's father, fitted to the human speech organs to ensure their “proper” operations in the production of verbal language human speech sounds.  

Standard Written English is (re)inscribed into Eliza’s “body” in new ways at every stage of her existence, even in the times before she had a name and a biography.  

She is originally made up of "shorthand" characters from a non-latin-alphabetic system of notation for a proposed universal alphabet developed by Alexander Melville Bell. These characters are then used by AGB in his research into the transportation of sound over telegraphic networks and in his work with the deaf.  They are later incorporated into the international phonetic alphabet and, via the work of the linguist Henry Sweet, are conceived as the character of Eliza by George Bernard Shaw.  Disabilities, labor, class, public literacy education, race, commerce, nation building, and technologies of reproduction and communication are part of Eliza's history and inform every stage of her existence, even those that occurred before she was born.   

How the stories of Eliza as a set of "shorthand" characters, as a fictional character whose story is told via stage, print, and film, and, eventually, as machine code in a digital computational natural language processing application relate to and inform machine learning tools for the automated generation of human readable alphabetic text is the broad subject of this article.


NOVEMBER 1, 2023

Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969

This is a project about the histories of instrumental symbolic inscription practices by humans and machines and their import to contemporary human to human, human to machine, and machine to machine communication theories, models, and practices. It documents and analyzes the dialectical relationships amongst information and communications technologies defined broadly as including technologies of print, social "technologies" of education (Bazerman), and definitions of verbal language to offer one reading of the "story" of how machines were taught to read and write in the U.S. from 1869 - 1969.  I use the verb "taught" here in a non-metaphorical sense, because the manner in which non-humans (mechanical, electric, and digital electronic devices and networks) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  This educative process has also involved the re-definition of a number of distinctly human attributes, processes, and practices in order to accommodate non-human devices as, first, audiences for, and, later, as participants in, the production and exhange of human readable signs.

The project constructs one narrative through an archive of documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up being in relation to, through a process of remediation, redefinition, and “translation,” in 1969 as a machine readable character in the Unix operating system to instruct a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, social, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  As a project about the histories of and relationships between instrumental symbolic inscription practices by humans and machines, it seeks to offer some clarification of terms and issues related to contemporary human to human, human to machine, and machine to machine communication. 

Introduction

This project documents and analyzes relationships between instrumental symbolic inscription practices by humans and machines from 1869, the year Alexander Melville Bell's ___ was published  to 1969, the year the Unix operating system was published by Bell Technical Labs.  


Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969 documents a genealogy of the relationships between human and non-human language and literacy education.  It constructs one narrative through an archive of documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” appearing in 1969 as a machine readable character in the Unix operating system instructing a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, social, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  As a project about the histories of and relationships between instrumental symbolic inscription practices by humans and machines, it seeks to offer some clarification of terms and issues related to contemporary human to human, human to machine, and machine to machine communication.




It proposes that there is some relationship between the representation of humans and their affordances as mechanisms and the personification of machines as humans and that the education of machines and the education of humans in the U.S. may exist in a dialectical relationship.  The two are inextricably bound up with one another.  It is a complicated story about humans, government administration, commerce, education, and nation building.  

Part of the nation's military budget was always being dedicated to literacy education.  It was used to teach machines to read and write; these processes were informed by the histories, theories, and practices of human literacy education.  The reasons for this are diverse and probably involve the relations of social technologies with material technologies.  I have only scratched the surface in this project and by making the materials I have used for this project available to other researchers, it is my hope that more will be revealed.  This is the story of latin alphabetic systems of notation being made to control/merged with non-latin alphabetic systems of notation.     

Textbooks, linguistics, public literacy education policy and practices, government spending, mechanical representation, reproduction, and distribution technologies, writing machines, human sciences, definitions of the human, figurative language, definitions of language are involved.  

This is a project about instrumentality and definitions of language in relation to human and machine communications.  This is a project about communication and instrumentality.  This project documents and analyzes relationships amongst the definitions and affordances of human and machine communication practices from 1869 - 1969. 

We have undoubtedly reached a new point in human/machine relations when a comparative literature major can begin to think of the latin alphabet as but “one system of notation” among others.  

How do engineers think about language?  Do they?  I’ve asked myself this question for some time and my attempt to answer the question may be the best way to describe this book and its purposes.   

Applied Linguistics 
The Manual Humanities
Sociologies of Literacy 

The potential impact of AI on literacy practices and literacy education, and the cognitive processes associated with them is one that; cognition and education are mediated by social and material technologies.  


By investigating the history of relationships between instrumentality and public literacy education [theories of and pedagogical approaches to verbal language] in the United States from the post-Civil War period to the post-World War II period and the dialectical relationshihps between humans and instruments mediated by verbal language in the same period, I document the ways in which instrumental uses of verbal language by humans has been instrumental in endowing electro-mechanical devices with the ability to manipulate human readable signs in a human like manner.  The reasons for the extended time period under investigation relate to the interconnections of 19th and 20th c. information and communications technologies and of the commercial and educational networks from the same period. 

This is a project about instrumentality and literacy.  It is also a book about humans and machines and their relations as mediated by symbolic systems used to encode verbal language.  

---


It constructs one narrative through an archive of documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” appearing in 1969 as a machine readable character in the Unix operating system instructing a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, political, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  

It is a project that seeks to construct a history of instrumentalist inscription practices in relation to nation building, public administration, education, information and communications technologies (commerce?), and media networks.  

This project presents one explanatory narrative path through a collection of archival documents to consider what processes are involved in verbal language becoming a domain of electronic digital computation. 

One approach to this question is by thinking of verbal language as a user interface with an implied "human" as adjective placed in front of "user."  

There has been and continues to be some excellent work being done in this area.  What distinguishes this project from those projects and from many other cultural histories of computation and information and communications technologies that have informed it is its attention to the roles of instruments as users in processes of verbal signification and their construction as symbolic systems over a one hundred year period. 

          
---

What connections might exist between the reclining caret in the Unix operating system command line and a printed character signifying the emission of breath accompanying the formation of a human speech sound in a nineteenth century printed manual of "shorthand"? How might an exploration of these connections help us to better understand the many roles and functions of technologies of inscription and their relations with writing and language education in the pasts, presents, and futures of computing machines?  While these may sound like intriguing if somewhat implausible hypotheses, this project seeks to explain how these two apparently separate realms of communication--human and machine--are related to one another and the disciplinary, technical, and socio-economic forces mediating their relationships.  

the connections between human and machine communication can be documented.  The story I plan to tell connects these two apparently disparate realms of communication (human/machine; print/digital) by tracing the remediation of spoken communication in Alexander Melville Bell’s system of "shorthand," which he called Visible Speech, and its afterlives as a series of material, medial, and socio-cultural artifacts in the technical, academic, and cultural imagination. 

What connections might exist between a symbol signifying the emission of breath used in the production of a spoken word in a printed diagram from an 1867 manual of shorthand by Herman Melville Bell and an identical symbol in the Unix operating system command line?  Both are  characters in a non-latin alphabetic systems of notation for the description and execution of an act of inscription.  That the Unix shorthand character instructs a digital electronic machine to act and the Visible Speech shorthand character instructs a human to act make their connections all the more intriguing.   

Could these graphical marks, despite their medial and temporal separation, be in some way related? 
Graphically, there is no doubt that they can be connected: the greater than sign ( > ), as it is familiarly called, is a very common typographical mark used in any number of non-latin alphabetic systems of notation and digital electronic applications.  But what about the provenance of these two specific tokens of this particular type?  As it turns out, they both act as instructions for acts of "communication" and both originate from work associated with Bell Technical Labs.  What is more, both marks are instructions for an act of inscription.  That the audience for the instruction in the Unix operating system is a machine while the audience for the character in Melville Bell's Visible Speech is humans whose speech organs are being represented as a mechanical system makes their connections all the more intriguing.    

if we define Unix as a kind of “digital shorthand,” as the New York Times did in explaining its operations to a lay audience in a 2011 obituary of Dennis Ritchie, one of the authors of the Unix operating system, the possible connections between these two marks become all that more intriguing (Lohr, NYTimes, October 11, 2011).  

Seeking to better understand the many roles and functions of technologies of inscription and their relationships with writing instruction in the pasts, presents, and futures of computing machines in the U.S., I plan to explore the historical connections between human and machine communication by analyzing six moments in the histories of human and machine communication and literacy "education."  the conditions that gave rise to Herman Melville Bell’s system of shorthand, which he called “Visible Speech,” its roles in the development of new communications technologies in the mid- and late- nineteenth century U.S., and its afterlives as a result of its influence and remediation in the technical, academic, and cultural imagination. 

Systems of shorthand, like other nineteenth-century innovations in the area of inscriptive practice and information and communications technologies (ICT), such as the telegraph, typewriter, phonograph, and telephone can be interpreted, as Lisa Gitelman has argued, as presenting their own unique “theories of language and textuality.”   

As composition instruction, writing practices, and scholarly activities in the twenty first century U.S. become more and more intertwined with digital technologies, the importance of understanding the genealogies of specific historical and contemporary ICTs, their intricate relations with the definitions and functions of verbal language, and the cultural, technical, and medial contexts in which they emerged have become increasingly important in the fields of the digital humanities and writing studies (Fuller, Haas, Heim, Kirschenbaum). 


Context


What the Artifacts Assume About Human Literacy


Findings



Engineering Language is a five act play with a nested structure:  
In the first act, speaking is re-defined as writing, i.e., an act of inscription and an unnatural attribute.  
In the second act, inscription is re-defined as "inscription," a writing system divorced from reality [?]. (Is this AGB or Peano?)
In the third act, communication is redefined as product or event, not process; it is something that can be described by an equation representing the limits/calculus of abstract mechanical inscription.
In the fourth act, electro mechanical calculators are re-defined as “minds”.
In the fifth act, electro mechanical calculators are re-defined as functionally “literate” devices.

This “play” begins and ends with shorthand “alphabets” as systems of notation that first enable humans to perform as instruments for other humans and that later enable machines to perform as instruments for humans formerly performing as instruments.  


This is a book about instrumental inscription practices by humans and machines and their relations.  It is a book about shorthand and longhand, their relations and the shifting boundaries between them in different medial environments/historical periods.  

Genre literature and the manual humanities (see Jennifer Wick 1992 article in Victorian Studies)


This is a book of forensic bibliography (Greetham) focused on a set of educational, technical and government documents for the purpose of better understanding the socio-cultural, technological, and economic currents shaping human and machine instrumental inscripton practices and their relationships in the U.S..  It constructs one narrative through an archival print archive spanning approximately one hundred years to explain how a mark from a book printed in 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” (re)appearing in 1969 as a machine readable character in the Unix operating system depicting an instruction for a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary archival investigation, the book considers how socio-cultural, technological, economic, political, and educational policies, currents, and issues are involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.   

The book is written for an interdisciplinary audience.  The purpose of the book is two fold: first, it seeks to make legible the dialectical relationships between human and machine language and literacy education for those working in education and the humanities and, secondly, to establish a framework and digital archive for further study by science, technology, society, and media history scholars of a history of instrumentalist inscription practices in nation building, public administration, education, information and communications technologies, and media networks in the United States from the end of the Civil War through the post-World World II period. 
 
The period and topics addressed in this book have been studied extensively.  The books that are most comparable to this book are David Mindell's 2002 _Between Human and Machine: Feedback, Control, and
Computing before Cybernetics_ and Matthew Kirschenbaum's ___ Mechanisms.  The work of Lorraine Daston, Lisa Gitleman, and Johanna Drucker have also been formative.  This book has been influenced by Gitleman's Scripts, Grooves, and Writing Machines, Digital Energetics, in This book distills and references dozens of cultural and historical studies of media and technologies in the history of the United States.   What distinguishes this project from others in the histories of media and science and technology studies is the dialogue established between these two fields and the history of literacy education and higher education in the United States.  Proposing that the literacy education of mechanical and mechanical-electronic devices exists in a dialectical relationship to human literacy education in the United States, the book seeks to map the intersections of human and “machine” education with a particular focus on the use, application, and management of human verbal language in the process.   

The book is a timely one for many reasons, one of the most obvious being its relations to the technologies and applications of web-based generative large language models as information and communications tools and applications. As “logical” extensions of text editing and processing applications combined with a deluge of virtually real time multimedia input (including alphabetic) and machine “learning” tools, generative LLMs like GPT3, Bard, ____, are the public manifestation of a fact that has been apparent for over two decades: Verbal language is a domain of computation and it has been since humans started communicating with machines.  

When I began my work on this research in 2014, I did so in order to understand the question of “How verbal language became a computational 'problem.'”   Somewhat to my surprise, though there was evidence all around me of how verbal language had become the domain of digital electronic computational systems, developing a plausible response to this question was complicated by numerous disciplinary factors, in particular the lack of any one discipline for studying the histories of writing and inscription practices.  Writing Systems, Writing Studies, Linguistics, Literacy Education, Composition and Rhetoric, the Digital Humanities

Harris: Linguistics
Heyck: Social Science
Dick: STS
Gitleman: Media Studies
___: STS/Communications
Kirschenbaum: Forensic Bibliography and Textual Studies
Lily Kay



Writing is a medium (how/process)
Writing is a conversation (why/rhetoric)
Writing is a symbolic system/currency (what/genre)

April 27, 2023

Instrumentality, Communication, and Systems of Notation

What are the affordances of the machine?

How is communication defined?

How is language defined?

How is verbal language defined?

How is information defined?

Moro_Syntax_and_the_brain.pdf




Ideas from The Prague School of functional linguistics have been adopted in NLP

Roman Jakobson writing about bits in his 1961 essay “Linguistics and Communication Theory” 

In this 1954 review of Willis’ book, the author proposes somewhat wistfully that the “inorganic world of mechanics” and the “immaterial realm of thought” might be brought together via a shared theoretical approach such as entropy and thermodynamics “Engineering.Language.Navy.Research.Lab.Review.Jackson.Communication.Theory.1953.1 

ChatGPT is one version of a mind, not a mind per se.  It is the “literate mind” that is discussed and described in the abstract and that has now been built thanks to a great deal of money and education.  

Project Overview

This project documents a genealogy of the relationships between human and non-human language and literacy education--and their relationships--from 1869 - 1969.   information and communications technologies defined broadly [as including technologies of print reproduction and social technologies (see Bazerman) and their relationships with definitions of language and practices of language education in the U.S. from 1869 - 1969.  Textbooks, linguistics, public literacy education policy and methods, government spending, mechanical representation, reproduction, and distribution technologies, writing machines, human sciences, definitions of the human, figurative language, definitions of language.  This is one version of the story of how machines were taught to read and write in the U.S.  I use the verb "taught" in a non-metaphorical sense, because the manner in which non-humans (mechanical and digital devices) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  The education of machines and the education of humans in the U.S. exists in a dialectical relationship.  The two are inextricably bound up with one another.  It is a complicated story about humans, government, and nation building.  Part of the nation's military budget was always being dedicated to literacy education.  It was used to teach machines to read and write instead of humans.  The reasons for this are diverse and probably involve the relations of social technologies with material technologies.  I have only scratched the surface in this project and by making the materials I have used for this project available to other researchers, it is my hope that more will be revealed.  This is the story of latin alphabetic systems of notation being made to control/merged with non-latin alphabetic systems of notation. 

ChatGPT is the latest instantiation of a complex of non-latin alphabetic systems of notation controlling the production and manipulation of latin alphabetic tokens, which humans usually refer to as words.  

medium = thing

mode (is this term always already a machine term?)

sense

cognition

The model used to teach digital electronic calculating machines to write was the current-traditional model of SWE education.  


Introduction

[What if] How might the reclining caret in the Unix command line can be in some way be connected to a mark signifying the emission of breath accompanying the formation of a spoken word?  What might that tell us about some of the many roles and functions of writing in the pasts, presents, and futures of computing machines?  While this may sound like an intriguing if somewhat implausible hypothesis, I hope to show that this connection between human and machine communication can be documented.  The story I would plan to tell provides one narrative connecting these two apparently disparate realms of communication (the human and machine) and in the process also traces connections amongst the spoken and the written, and the manual, mechanical, and digital, in ways that may offer some insights into the relationships amongst old and new technologies of inscription and representation and the social and material technological systems supporting them.  

First, there is the questions of what issues need to be addressed in establishing a framework between human inscription and representation practices and mechanical inscription and representation "practices."  


Confirmation: #17759123US4






Although the book is busy enough with the histories of verbal signification and computation to not have much time for a consideration of the genealogies of “Artificial Intelligence,” it is implicitly connected to those technologies and narratives through its concern with alphabetic and machine literacies.  As Stephanie Dick has written in her work on the histories of Artificial Intelligence, “making up minds,” is, from a technical and cultural perspective, very much the concern of current media representations and technical implementations of what is now referred to generally as “AI.”   

0. Introduction: Eliza Doolittle's Ghosts/ Introduction: Hermeneutics and Semiology: Exploring Relationships Between What and How Stories About Reading and Writing Are Told

1. Was AGB Raised as a Telephone? Alexander Melville’s Visible Speech as an Operational System of Notation and Educational Standard:

2. Latin for the Masses: Mechanical Minds, Universal Languages and Markov’s Characters, or Logic and Statistics as Reading and Writing Instruments:

3. Telemental Models of Communication: Hartley’s Equation and Saussure’s Wires:

4. Before the Byte, There Was the Word: Von Neumann’s Word Choice

5. Hopper’s Syllabus and Tukey’s Neologism

6. >, or How Processed Do You Like Your Humans: Thompson and Ritchie’s Unix Operating System as Electronic “Shorthand” 



There is a lot that is not in this book that it is my hope that other scholars and historians will further investigate these themes and topics using the digital archive and bibliography accompanying this project.  

This project is also imbricated with my own literacy narrative, which began in the American midwest in 1968, was accredited first at Stanford University in 1990 when I graduated as a Comparative Literature major, developed in an applied professional capacity in Silicon Valley after my graduation from college, and broadly expanded by my work as a professional writer in New York City and my research and teaching with students at The City University of New York.  I do not hesitate these days to say that I have made my living as a professional in high tech, first in the private sector and later in the public sector.  High tech has become a part of every industry in the last thirty years, so this should not come as any great surprise.  However, what I did not understand until becoming more involved with the histories of computings was how much my educational training in philosophy, linguistics, writing, and textual studies had to do with engineering and its various disciplines.    

 
The problems this project seeks to address include:

1/ How digital electronic networked computational devices learned how to read and write latin alphabetic characters, the discreet stages of this process, and the relationships between the technical engineering of material artifacts and of socio-cultural systems at each stage. 

2/ Why and how Standard Written English as a dialect becomes embedded in digital electronic networked computational devices and the socio-cultural and educational import of this. 

3//How rhetoric functions in technical documents to negotiate the boundaries between human and machine instrumentality and to promote the interests of some parties and interests over others.   

To address these issues, several popular and disciplinary myths need to be addressed: 

1/ That digital electronic computing systems have always been symbolic systems that “educated” themselves through automated and systematic ingenuity.
  
2/ That humanist analyses and critiques of human languages and their socio-cultural functions are of no import to the technical or applied applications of human and machine languages.   

3/ That the simultaneous deployment of technical and instrumental uses of verbal language is not an event of some significance in the histories of computing and the histories of education in the U.S.. 

4/ That the expenditures on the higher literacy education of machines has been both inevitable and had anything but beneficial effects on the processes and purposes of human literacy education.   

* What previously unknown or unfortunately neglected story are you planning to tell?
How machines learned to read and write in the U.S. from 1869 - 1969 and how this “literacy narrative” is interwoven with the socio-economic, medial, technical, cultural, political, and educational currents of the same period.  [Comparing and contrasting the rise of mechanical and digital electronic literacies and human literacies, the dialectical relationships between the two are explored.]     

This book is about the histories of systems of notation and their import to writing studies and literacy education.

* How is this book different from all other books?

It defines its difference from other books as provisional and invites scholarly collaboration in the discussion and research.  It presents its argument as one interpretation of a set of documentary evidence published in a shared digital archive for other scholars to use and expand upon.

It provides a rational for the field of the Manual/Applied Humanities/Critical Bibliography and Textual Studies by considering a history of instrumental inscriptive practices as including both social technologies (public literacy education, government funded initiatives related to defense, commerce, education, and administration, definitions and implementations of language in a range of higher educational disciplines) and material technologies (shorthand “alphabets,” printing presses, telegraphs, telephones, typewriters, teletypewriters, adding machines, cash registers, vacuum tubes, machine languages, keyboards, memory units, semiconductor chips, software systems and applications)  and traces the meeting of these two technological domains at specific moments in time based on evidence from discursive artifacts in government and corporate archives and publications.

It is written for an interdisciplinary audience of writing educators, cultural critics, historians, engineers, and the general public interested in cultural histories of computational devices in the United States.  It aspires to synthesize the arguments of forty years of socio-cultural studies on computation and its relationships with literacy education practices and policies for a general audience.  It is an interdisciplinary documentary project aimed at synthesizing existing historical and cultural work on the histories of computation.  

It is relevant to contemporary issues and events such as networked llms and their implications for human literacy practices and education. It suggests that computational technologies and their relationships with the socio-cultural, environmental, educational, and economic interests of the public need to be managed in the interests of something other than the logic of technics (Winner) as it has come to be defined in the early twenty-first century, i.e., efficiency and cost savings, to protect the well being of biological organisms, including humans.   

How, when, and why did digital electronic networked computational devices become symbolic processing machines and how, when, and why do assumptions about and practices related to symbolic “processing” by humans inform the “literacy narratives” of these mechanical and electronic calculators and communications devices?  

XXXXXXXXXXX
 

It is collaborative and provisional. 

Import

Audience


* Why does that matter? To whom?
Possible audiences are as variable as publishers. Consider:
* Is your book for specialists in your field?  It is for scholars working in the interdisciplinary fields of media archaeology, the manual humanities, software studies, science and technology studies
* Does your book focus on a particular area within a larger field?  
* Is it a book that students might use, and if so, students at what level?  It would be useful to graduate students in education, writing studies, STS.
* Is it a “trade” book? That is, one intended for general readers, those without specialized knowledge in your area?
Whatever your answer, consider carefully the kind of approach, terminology, level of explanation, and scholarly apparatus that your book will need to make it most compelling for your ideal reader.
Successful proposals usually include:
* A narrative description of the proposed book’s themes, arguments, goals, place in the literature, and expected audience. State your argument concisely and clearly.

How, when, and why did digital electronic networked computational devices become symbolic processing machines and how, when, and why do assumptions about and practices related to symbolic “processing” by humans inform the “literacy narratives” of these mechanical and electronic calculators and communications devices?  

There are  several complications in posing this question in this form.  The first set of complications arise with the determined persistence with which the myth that electronic computational devices sometime around 1950 simply arose as symbol processing machines (Dreyfus and Dreyfus 1988).  This is an ahistorical depiction of the emergence of these machines that has been reinforced by media networks and corporate interests.  These devices and the social and material technologies embedded in them have histories that have been well documented by scholars in a range of fields.  Yet, the dominant media and commercial narratives of symbolic processing and its applications, what is now generally categorized under the name of artificial intelligence, is ahistorical.   

The second set of complications arise in relation to the breadth and depth of the disciplinary specialization and technical expertise required to respond to a question involving the histories of so many different disciplines: computational devices, mathematics, psychology, brain science, linguistics, and engineering.  Yet, within engineering, applied versions of each of these disciplines are used in research and development of products and technologies and in 1986, this interdisciplinary field was given a name and an institutional role at Stanford University with the creation of the Symbolic Systems program.  While it is not possible to publish a scholarly book that encompasses the histories of the disciplines of linguistics, psychology, cognitive science, neurosurgery, and public education, applied versions of each of these disciplines are borrowed and freely adapted in the development of products and new technologies related to human language processing.  While there is an interdisciplinary field called “Symbolic Systems,” its history has been written in any number of disciplines making up the field.  This project suggests that this field of Symbolic Systems requires a history that includes a consideration of the material and the social technologies of signification and symbol making via archival textual evidence.      
The histories of public literacy education and the histories of machine “literacy” in the United States share an identical timeline.  Yet the two are rarely/surprisingly brought into dialogue with one another in relation to their shared concern with social and material technologies of inscription, or what is sometimes referred to as “writing.”  The term writing is problematic because it signifies so many different things depending on the context in which it is deployed.  It can signify making a marks on a page, the graphic depiction of speech, _____, or ______.   This term “writing,” whether being used as a synonym or synechdoche for literacy is one unique both in its vagueness/generality and scope/breadth.  Particularly in a socio-educational context this term “writing” as it relates to advanced literacy in Standard Written English is integral to a range of educational, social, cultural, and commercial operations and functions.  Instrumental uses of social and material technologies of inscription.  
                     
Having dedicated most of my education and professional life to understanding how humans read and write and the import of both acts, I also, because of the economic realities of my life and job market, spent ten years of my professional life becoming conversant in how electronic digital computers operate.  The interconnections and relationships between the literacy practices of both have been addressed by scholars in the social sciences and STS in different disciplines but never from the interdisciplinary perspective of instrumental literacy and signification functions.  
The scholarship on the roles of technologies in K-12 and higher ed writing instruction are distinct.  Until the early 2000s, there was surprisingly little scholarship in higher ed writing studies dedicated to discussions of writing technologies.  Christina Haas Writing Technology: Studies on the Materiality of Literacy (1996) is one exception.  Charles Bazerman’s Languages of Edison’s Light (1999) may be another though this book is usually categorized as being part of the history of science and technology studies rather than as part of the history of composition.  


Lorraine Daston: a history of inscriptive practices and technologies in Nation Building: Writing Studies, Literacy Education, Media Archaeology.  The Manual Humanities, which prioritizes the needs and specificities of human cognitive processes, human labor practices, and humans as a species in a highly delicate and interconnected global physical, social, biological, and geological environment.              
* A comparison of the proposed book to other books now available that are intended for the audience you seek. (If you are writing a specialized monograph, it is not especially illuminating to compare it to a popularized treatment of the same subject.)
David Golumbia
Matthew Fuller
Wendy Chung
Anne Pasek, et al
Cubitt, Sean. 2017. Finite Media: Environmental Implications of Digital Technologies. Durham, N.C.: Duke University Press.
* A summary of your own professional experience, past publications, and relevant research, aimed at explaining why you are the right author for the book you intend to write.


My intellectual, creative, and professional lives have been shaped by the intersection of the medial, technical, and economic realities in which I was raised and in which I have lived as part of a specific generation of well educated humans in the United States.  Born in 1968, I was raised and educated in Ann Arbor, Michigan, where I attended public schools.  First, Burns Park Elementary School, which was one of the first public elementary schools built by the newly formed Department of Education in the U.S. Later, Tappan Middle School, named after _____, and finally Pioneer High School. 

I would like humans who do not currently work in Engineering  and the disciplines related to it to know, first, that human language, its use, and its management has had a lot to do with how and why decisions have been made to prioritize the interests of electronic computational devices and the systems supporting them over those of socio-cultural and environmental conservation, and, second, that, regardless of their disciplinary background and level of education, they are qualified to engage in conversations related to decisions about the priorities for public policy and public spending related to civic and educational technological investment that will be made in the next ten to twenty years.         

My own literacy, educational, and professional narrative exists in a metonymic relationship with the story told in this book. Remediation, disciplinary agonism, and the roles of capital in higher education are part of what makes me uniquely qualified to write this book.  [Few people other than me could write this book because the story is in part a refraction of my own literacy and educational narrative.]  


Even in the early 1970s cultures of print were already facing off with new digital electronic technologies, one material trace of which was the x by y punch cards inside the front cover of every book sold at the local university bookstore, Borders.  At that time, one of the two Borders brothers was enrolled at the University of Michigan and had access to the mainframe computer.  He used the computing time granted to him to write a program for the inventory management of the bookstore.  Each night, ____ would collect the data cards from that day’s sales and update the store’s inventory.  I have only third hand information about the specific programs used, but I know that this early (1972) management and integration of sales and inventory systems were in part what led to the success of Borders Bookstore both as an independent business and later as a chain that was acquired by KMart.

I graduated from high school in 1986 and attended Stanford University as an undergraduate on a full scholarship, majoring in Comparative Literature.  Why I did not study computer science is, in the 21st century, hard for me to comprehend.   However at the time, this decision had to do with how the subject was taught –and largely still is taught– that I was categorized as a “verbal,” not a “math” person, and that the humanities, particularly textual studies, still appeared to me to be highly relevant to society and culture.  

Having been provisionally hired by the first management consulting firm in Silicon Valley in 1992 after I took a leave of absence from a Ph.D. Program in Comparative Literature at Yale University and was unable to find a job in print publishing in the San Francisco Bay Area, I spent the next ten years researching new technologies, primarily database software, in an office park shared by Sun Microsystems.  At that time, I never once considered learning Unix or SqlServer even though I discussed that operating system and database scripting language daily with the IT managers whom I interviewed and the marketing, communications, and strategic development officers at the then F1000 technology companies who were our clients.  I was privy, in 1995, to early discussions about the launch of Adobe’s Portable Document Format standard, which I would only come to understand in the 2020s as the corporate standardization of a publicly available file standard.  My colleagues at the management consulting firm, most of whom had attended Yale rather than Stanford, and who therefore knew what it actually meant to be a “Comparative Literature major” were amused, astonished, and pleased at how well a degree in philosophy, linguistics, and textual studies had prepared me to work as a research analyst in the high tech industry.    

In 1995, I relocated to Brooklyn, New York, continued to work remotely for the management consulting firm in Silicon Valley, and enrolled in a graduate creative writing program at the City College of New York.  It was by completing this program that I first became an adjunct writing instructor at the City University of New York, where I would complete my Ph.D. in English/Composition and Rhetoric in 2007.  I first enrolled at CCNY because I aspired to have a career in creative writing and I chose to attend a degree program I could easily afford.  The tuition at CUNY has since 1995 quadrupled.  My experiences in public higher ed have been, both as a student and as an instructor, decidedly mixed.  I believe in the mission of public education and yet I was also aware that I was treated much better at a private college.      

 

* An annotated table of contents, with a brief description of the contents of each chapter.
* An estimate of the probable length of the book, the illustrations (if any) that you wish to include, the time it will take you to write it, and any possible complicating factors.

Please send all proposals by email to submissions_HUP@harvard.edu (Attention: Editorial). You should expect an answer within three to four weeks. If you choose to send your proposal by U.S. mail or courier please be advised that we are not responsible for lost packages and we do not return unsolicited manuscripts or proposals.

Writing Studies

Linguistics

Semiotics

STS/Computer History

STS/Inscriptive Practices: Donna Haraway, Lily Kay, Friedrich Kittler, Lisa Gitelman, Johanna Drucker, Stephanie Dick, Chung, what's his face, Charles Bazerman, Sean Cubitt, 

Media Studies and Archaeology

Bibliography

Composition and Rhetoric/Communication Studies

Digital Humanities

NOVEMBER 4, 2023

Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969

This is a project about the histories of and relationships amongst instrumental symbolic inscription practices and systems by humans and machines and its import to contemporary human to human, human to machine, and machine to machine communication practices, theories, and models. It documents and analyzes the relationships amongst information and communications technologies defined broadly as including technologies of print, social "technologies" of literacy education (Bazerman), and definitions of verbal language to offer one reading of the "story" of how machines were taught to read and write in the U.S. from 1869 - 1969.  I use the verb "taught" here in a non-metaphorical sense, because the manner in which non-humans (mechanical, electric, and digital electronic devices and networks) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  This educative process has also involved the re-definition of a number of distinctly human attributes, processes, and practices in order to accommodate non-human devices as, first, audiences for, and, later, as participants in, the production and exhange of human readable signs.

The project constructs one narrative through an archive of technical, business, educational, and government documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” (re)appearing in 1969 as a machine readable character in the Unix operating system to instruct a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, social, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  As a project about the histories of and relationships amongst instrumental symbolic inscription practices by humans and machines, it seeks to elucidate specific moments in the dialectical relationships between latin alphabetic and non-latin alphabetic systems of notation and their mediation via higher education disciplines, military spending, literacy education practices, and the tools and technologies of representation, duplication, and inscription. 



Seeking to better understand the many roles and functions of ICTs/technologies of inscription and their relationships with writing instruction in the pasts, presents, and futures of computing machines in the U.S., I plan to explore the historical connections between human and machine communication by analyzing six moments in the histories of human and machine communication and literacy "education."

Systems of shorthand, like other nineteenth-century innovations in the area of inscriptive practice and information and communications technologies (ICT), such as the telegraph, typewriter, phonograph, and telephone can be interpreted, as Lisa Gitelman has argued, as presenting their own unique “theories of language and textuality.”   

As composition instruction, writing practices, and scholarly activities in the twenty first century U.S. become more and more intertwined with digital technologies and semiotics, the importance of understanding the genealogies of specific historical and contemporary ICTs, their intricate relations with the definitions and functions of human languages, and the cultural, technical, and medial contexts in which they emerged have become increasingly important in textual and writing studies (Fuller, Haas, Heim, Kirschenbaum). 

Engineering Language is a five act play with a nested structure:  
In the first act, speaking is re-defined as writing, i.e., an act of inscription and an unnatural attribute of humans.  
In the second act, inscription as an act in a human built world is re-defined as "inscription," or an anological act, i.e., action via analogy, and/or as a non-human act in a parallel electrical universe an electric writing system separate from physical reality [?]. (Is this AGB or Peano?)
In the third act, communication is redefined as product or event, not process; it is something that can be described by an equation representing the limits/calculus of abstract mechanical inscription.
In the fourth act, electro mechanical calculators are re-defined as “minds” that process "words".
In the fifth act, electro mechanical calculators are re-defined as "students" and programmed via scripts to become functionally “literate” devices.


This “play” begins and ends with shorthand “alphabets” as systems of notation that first enable humans to perform as instruments for other humans and that later enable machines to perform as instruments for humans formerly performing as instruments.  


This is a book about instrumental inscription practices by humans and machines and their relations.  It is a book about systems of shorthand and writing education, technical and educational standards, communications technologies, and military supported literacy and language research, their relations and the shifting boundaries between them in different medial environments/historical periods.  

0.0 Prologue: Eliza Doolittle's Ghosts, or Mapping Relationships Amongst Hermeneutics and Semiology in the U.S.  // an Introduction to Hermeneutics and Semiology in Writing Studies in the U.S.: Exploring Relationships Between What and How Stories About Reading and Writing Are Told

The character who will eventually become Eliza Doolittle in the 1913 play “Pygmalion” by George Bernard Shaw and in the 1956 musical adaptation “My Fair Lady” and in the 1966 machine code of the computer program ELIZA began her life as a set of instructions for the production of human speech sounds in 1865.  Before she was ever a character, she was a set of graphic characters that Alexander Melville Bell fitted to the human speech organs to ensure their “proper” operations.  Standard Written English is (re)inscribed into Eliza’s “body” in new ways at every stage of her existence, even in the times before she had a name and a biography.  She is originally made up of "shorthand" characters from a non-latin-alphabetic system of notation for a proposed universal alphabet developed by Alexander Melville Bell. These characters are then used by AGB in his research into telegraphic communication of sound and in his teaching.  They are later incorporated into the international phonetic alphabet and, via the work of the linguist Henry Sweet, are conceived as the character of Eliza by George Bernard Shaw.  Disabilities, labor, class, public literacy education, race, commerce, nation building, and technologies of reproduction and communication are part of Eliza's history and inform every stage of her existence, even those that occurred before she was born.   

How the stories of Eliza as a set of "shorthand" characters, as a fictional character whose story is told via stage, print, and film, and, eventually, as machine code in a digital computational natural language processing application relate to and inform ChatGPT and other generative AI tools for the automated generation of human readable alphabetic text is the broad subject of this book.  

1. Was AGB Raised as a Telephone? Alexander Melville’s Visible Speech as, System of Notation, "Operating System" for Humans, and Educational "Standard"

The answers to this question depend first, of course, on your definitions of AGB and of the telephone. Both are complicated subjects. However, if you consider each as having functioned at various points in time as instruments for the production, replication, and transportation of graphic representations of human speech sounds, one answer to my apparently anachronistic question is "quite possibly."  Although the telephone may be an object more often conceived of as an instrument than young Scottish born men being trained to carry on the family business of elocutionary education, as it turns out, AGB was likewise employed for instrumentalist purposes.  Raised speaking several different dialects of English and Scottish, AGB was also a fluent speaker and writer in his father's non-latin alphabetic system of notation named "Visible Speech."  This chapter considers the long standing question of exactly what AGB "invented," by proposing that both "an analogy," instrumental access to a non-human time/space continuum, and a new way of writing should perhaps be added to the list.  As a great deal of scholarship over the last fifty years has shown, the telephone was not invented by one individual.  However, the import of AGB's work persists in its lasting traces in institutional structures of the Harvard Composition Program and at Bell Technial Labs.     

2. Latin for the Masses: Mechanical Minds, Universal Languages and Markov’s Characters, or Logic and Statistics as Reading and Writing Instruments. Boole, Education, Peano, Phonetic Alphabet

3. Telemental Models of Communication: Hartley’s Equation and Saussure’s Wires

4. Before the Byte, There Was the Word: Von Neumann’s Word Choice

5. "The Computer Is an Extremely Fast Moron": Hopper’s Syllabus and Tukey’s Neologism

6. >, or How Processed Do You Like Your Humans: Thompson and Ritchie’s Unix Operating System as System of Notation and Electronic “Shorthand” 


NOVEMBER 9, 2025

Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969

Documenting and analyzing six key moments in the relationships amongst information and communications technologies defined broadly and definitions of verbal language, the project presents one reading of the "story" of how machines were taught to "read" and "write" in the U.S. from 1869 - 1969 by constructing one narrative through collection of educational and technical documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” reappearing in 1969 as a machine readable character in the Unix operating system instructing a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, political, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  It is a project that attempts to construct a history of instrumentalist inscription practices in nation building, public administration, education, information and communications technologies, and media networks. 

This project constructs one narrative through an archive of educational and technical documents spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” (re)appearing in 1969 as a human and machine readable character in the Unix operating system to instruct a digital electronic computing machine to write the contents of one file to another file location.

Documenting and analyzing six key moments in the relationships amongst information and communications technologies defined broadly and definitions of verbal language, the project presents one reading of the "story" of how machines were taught to read and write in the U.S. from 1869 - 1969.  I use the verb "taught" here in a non-metaphorical sense, because the manner in which non-humans (mechanical, electric, and digital electronic devices and networks) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  This educative process has also involved the re-definition of a number of distinctly human attributes, processes, and practices in order to accommodate non-human devices as, first, audiences for, and, later, as participants in, the production and exhange of human readable signs.  As a project about the histories of and relationships amongst instrumental symbolic inscription practices by humans and machines, it seeks to elucidate specific moments in the dialectical relationships between latin alphabetic and non-latin alphabetic systems of notation and between humans and machines and their mediation via higher education disciplines, military spending, literacy education practices, and the tools and technologies of representation, duplication, and inscription.

  

Through this documentary investigation, the project considers the many socio-cultural, medial, technological, economic, disciplinary, and educational policies, currents, and forces, and the dynamics of signification supporting each in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  





 



Seeking to better understand the many roles and functions of ICTs/technologies of inscription and their relationships with writing instruction in the pasts, presents, and futures of computing machines in the U.S., I plan to explore the historical connections between human and machine communication by analyzing six moments in the histories of human and machine communication and literacy "education."

Systems of shorthand, like other nineteenth-century innovations in the area of inscriptive practice and information and communications technologies (ICT), such as the telegraph, typewriter, phonograph, and telephone can be interpreted, as Lisa Gitelman has argued, as presenting their own unique “theories of language and textuality.”   

As literacy instruction, writing practices, and scholarly activities in the twenty first century U.S. become more and more intertwined with digital technologies and semiotics, the importance of understanding the genealogies of specific historical and contemporary ICTs, their intricate relations with the definitions and functions of human languages, and the cultural, technical, and medial contexts in which they emerged have become increasingly important in textual and writing studies (Fuller, Haas, Heim, Kirschenbaum). 

Engineering Language is a five act play with a nested structure:  
In the first act, speaking is re-defined as writing, i.e., an act of inscription and an unnatural attribute of humans.  
In the second act, inscription as an act in a human built world is re-defined as "inscription," or an anological act, i.e., action via analogy, and/or as a non-human act in a parallel electrical universe an electric writing system separate from physical reality [?]. (Is this AGB or Peano?)
In the third act, communication is redefined as product or event, not process; it is something that can be described by an equation representing the limits/calculus of abstract mechanical inscription.
In the fourth act, electro mechanical calculators are re-defined as “minds” that process "words".
In the fifth act, electro mechanical calculators are re-defined as "students" and programmed via scripts to become functionally “literate” devices.


This “play” begins and ends with shorthand “alphabets” as systems of notation that first enable humans to perform as instruments for other humans and that later enable machines to perform as instruments for humans who then instrumentalize other humans in different ways.  


This is a book about instrumental inscription practices by humans and machines and their relations.  It is a book about systems of shorthand and writing education, technical and educational standards, communications technologies, and military supported literacy and language research, their relations and the shifting boundaries between them in different medial environments/historical periods.  

0.  Writing Studies and Systems of Notation: Reinscribing Inscription as an Instrument in a Semiotics of Human and Machine Communication   

1. Was AGB Raised as a Telephone? Alexander Melville’s Visible Speech as, System of Notation, "Operating System" for Humans, and Educational "Standard"

The answers to this question depend first, of course, on your definitions of AGB and of the telephone. Both are complicated subjects. However, if you consider each as having functioned at various points in time as instruments for the production, replication, and transportation of graphic representations of human speech sounds, one answer to my apparently anachronistic question is "quite possibly."  

While the telephone may be an object more often conceived of as an instrument than young Scottish born men being trained to carry on the family business of elocutionary education, as it turns out, AGB was likewise employed for instrumentalist purposes.  Raised speaking several different dialects of English and Scottish, AGB was also a fluent speaker and writer in his father's non-latin alphabetic system of notation named "Visible Speech."  This chapter considers the long standing question of exactly what AGB "invented," by proposing that part of what he invented was "an analogy" of humans and machines as inscriptive devices. instrumental access to a non-human time/space continuum, and a new way of writing should perhaps be added to the list.  As a great deal of scholarship over the last fifty years has shown, the telephone was not invented by one individual.  However, the import of AGB's work persists in its lasting traces in institutional structures of the Harvard Composition Program and at Bell Technial Labs.     

2. Latin for the Masses: Mechanical Minds, Universal Languages and Markov’s Characters, or Logic and Statistics as Reading and Writing Instruments. Boole, Education, Peano, Phonetic Alphabet

3. Telemental Models of Communication: Hartley’s Equation and Saussure’s Wires

4. Before the Byte, There Was the Word: Von Neumann’s Word Choice

5. "The Computer Is an Extremely Fast Moron": Hopper’s Syllabus and Tukey’s Neologism

6. >, or How Processed Do You Like Your Humans: Thompson and Ritchie’s Unix Operating System as System of Notation and Electronic “Shorthand” 

Epilogue: Eliza Doolittle's Ghosts, or Mapping Relationships Amongst Hermeneutics and Semiology in the U.S.  // an Introduction to Hermeneutics and Semiology in Writing Studies in the U.S.: Exploring Relationships Between What and How Stories About Reading and Writing Are Told, or Inscriptive Media, Machine Learning, Translation, and Artificial Languages

The character who will eventually become Eliza Doolittle in the 1913 play “Pygmalion” by George Bernard Shaw and in the 1956 musical adaptation “My Fair Lady” and in the 1966 machine code of the computer program ELIZA began her life as a set of instructions for the production of human speech sounds in 1865.  Before she was ever a character, she was a set of graphic characters that Alexander Melville Bell, Alexander Graham Bell's father, fitted to the human speech organs to ensure their “proper” operations in the production of verbal language human speech sounds.  

Standard Written English is (re)inscribed into Eliza’s “body” in new ways at every stage of her existence, even in the times before she had a name and a biography.  

She is originally made up of "shorthand" characters from a non-latin-alphabetic system of notation for a proposed universal alphabet developed by Alexander Melville Bell. These characters are then used by AGB in his research into the transportation of sound over telegraphic networks and in his work with the deaf.  They are later incorporated into the international phonetic alphabet and, via the work of the linguist Henry Sweet, are conceived as the character of Eliza by George Bernard Shaw.  Disabilities, labor, class, public literacy education, race, commerce, nation building, and technologies of reproduction and communication are part of Eliza's history and inform every stage of her existence, even those that occurred before she was born.   

How the stories of Eliza as a set of "shorthand" characters, as a fictional character whose story is told via stage, print, and film, and, eventually, as machine code in a digital computational natural language processing application relate to and inform machine learning tools for the automated generation of human readable alphabetic text is the broad subject of this article.

MAY 23, 2023

Engineering Language: Teaching Machines to Read and Write in the U.S. 1869 - 1969

Introduction

The character who will eventually become Eliza Doolittle in the 1913 play “Pygmalion” by George Bernard Shaw and in the 1956 musical adaptation “My Fair Lady” and in the 1966 machine code of the computer program ELIZA began her life as a set of instructions for the production of human speech sounds in 1865.  Before she was ever a character, she was a set of graphic characters that Alexander Melville Bell fitted to the human speech organs to ensure their “proper” operations.  Standard Written English is written into Eliza’s “body” in new ways at every stage of her existence, even in the times before she had a name and a biography.  She is originally made up of shorthand symbols from a non-latin-alphabetic system of notation for a proposed universal alphabet.  The characters and eventually Eliza's character are given life by being applied to the human speech organs.  Disabilities, labor, class, public literacy education, race, commerce, nation building, and technologies of reproduction are part of Eliza's history and inform every stage of her existence, even those that occurred before she was born. 

How the stories of Eliza as a set of "shorthand" characters, as a fictional character whose story is told via stage, print, and film, and, eventually, as machine code for digital computational natural language processing relate to and inform ChatGPT and other generative AI tools for the automatic composition of human readable alphabetic text is the broad subject of this book.  

By investigating the history of relationships between instrumentality and public literacy education [verbal language] in the United States from the post-Civil War period to the post-World War II period and the dialectical relationshihps between humans and instruments mediated by verbal language in the same period, I document the ways in which instrumental uses of verbal language by humans has been instrumental in endowing electro-mechanical devices with the ability to manipulate human readable signs in a human like manner.  The reasons for the extended time period under investigation relate to the interconnections of 19th and 20th c. information and communications technologies and of the commercial and educational networks from the same period. 

This is a project about instrumentality and literacy.  It is also a book about humans and machines and their relations as mediated by symbolic systems used to encode verbal language.  

This is a book of media archaeology and forensic bibliography in their most literal definitions.  

It constructs one narrative through an archival print archive spanning approximately one hundred years to explain how a printed graphic mark from 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” reappearing in 1969 as a machine readable character in the Unix operating system instructing a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary investigation, the book considers the many socio-cultural, technological, economic, political, and educational policies, currents, and issues involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.  

It is a project that attempts to construct a history of instrumentalist inscription practices in nation building, public administration, education, information and communications technologies, and media networks.  

This project presents one explanatory narrative path through a collection of archival documents to consider what processes are involved in verbal language becoming a domain of electronic digital computation? One approach to this question is by thinking of verbal language as a user interface with an implied "human" as adjective placed in front of "user."  There has been and continues to be some excellent work being done in this area.  What distinguishes this project from those projects and from many other cultural histories of computation and information and communications technologies that have informed it is its attention to the roles of instruments as users in processes of verbal signification and their construction as symbolic systems over a one hundred year period. 

          



Context


What the Artifacts Assume About Human Literacy


Findings



Engineering Language is a five act play with a nested structure:  
In the first act, speaking is re-defined as inscription.  
In the second act, inscription is re-defined as a writing system divorced from reality [?]. (Is this AGB or Peano?)
In the third act, communication is redefined as product or event, not process; it is something that can be described by an equation representing the limits/calculus of abstract mechanical inscription.
In the fourth act, electro mechanical calculators are re-defined as “minds”.
In the fifth act, electro mechanical calculators are re-defined as functionally “literate” devices.

This “play” begins and ends with shorthand “alphabets” as systems of notation that first enable humans to perform as instruments for other humans and that later enable machines to perform as instruments for humans formerly performing as instruments.  


This is a book about instrumental inscription practices by humans and machines and their relations.  It is a book about shorthand and longhand, their relations and the shifting boundaries between them in different medial environments/historical periods.  

Genre literature and the manual humanities (see Jennifer Wick 1992 article in Victorian Studies)


This is a book of forensic bibliography (Greetham) focused on a set of educational, technical and government documents for the purpose of better understanding the socio-cultural, technological, and economic currents shaping human and machine instrumental inscripton practices and their relationships.  It constructs one narrative through an archival print archive spanning approximately one hundred years to explain how a mark from a book printed in 1869 depicting an instruction for the emission of breath associated with the production of a human speech sound ends up, through a process of remediation, redefinition, and “translation,” (re)appearing in 1969 as a machine readable character in the Unix operating system depicting an instruction for a digital electronic computing machine to write the contents of one file to another file location.  Through this documentary archival investigation, the book considers how socio-cultural, technological, economic, political, and educational policies, currents, and issues are involved in the process by which digital electronic computing machines were endowed with the ability to read and write latin alphabetic characters.   

The book is written for an interdisciplinary audience.  The purpose of the book is two fold: first, it seeks make visible the dialectical relationships between human and mechanical literacies for those working in education and the humanities and, secondly, to establish a framework and digital archive for further study of a history of instrumentalist inscription practices in nation building, public administration, education, information and communications technologies, and media networks in the United States from the end of the Civil War through the post-World World II period. 
 
The period and topics addressed in this book have been studied extensively.  The books that are most comparable to this book are David Mindell's 2002 _Between Human and Machine: Feedback, Control, and
Computing before Cybernetics_ and Matthew Kirschenbaum's ___ Mechanisms.  The work of Lorraine Daston, Lisa Gitleman, and Johanna Drucker have also been formative.  This book has been influenced by Gitleman's Scripts, Grooves, and Writing Machines, Digital Energetics, in This book distills and references dozens of cultural and historical studies of media and technologies in the history of the United States.   What distinguishes this project from others in the histories of media and science and technology studies is the dialogue established between these two fields and the history of literacy education in the United States.  Proposing that the literacy education of mechanical and mechanical-electronic devices exists in a dialectical relationship to human literacy education in the United States, the book seeks to map the intersections of human and “machine” education with a particular focus on the use, application, and management of human verbal language in the process.   
The book is a timely one for many reasons, one of the most obvious being its relations to the technologies and applications of web-based generative large language models as information and communications tools and applications. As “logical” extensions of text editing and processing applications combined with a deluge of virtually real time multimedia input (including alphabetic) and machine “learning” tools, generative LLMs like GPT3, Bard, ____, are the public manifestation of a fact that has been apparent for over two decades: Verbal language is a domain of computation and it has been since humans started communicating with machines.  

When I began my work on this research in 2014, I did so in order to understand the question of “How verbal language became a computational problem.”   Somewhat to my surprise, though there was evidence all around me of how verbal language had become a computational “problem,” developing a plausible response to this question was complicated by numerous disciplinary and technical factors.  



Writing is a medium (how/process)
Writing is a conversation (why/rhetoric)
Writing is a symbolic system/currency (what/genre)

April 27, 2023

Instrumentality, Communication, and Systems of Notation

What are the affordances of the machine?

How is communication defined?

How is language defined?

How is verbal language defined?

Moro_Syntax_and_the_brain.pdf


This project is about [establishing the continuity in] the genealogy of information and communications technologies defined broadly [as including technologies of print reproduction and social technologies (see Bazerman) and their relationships with definitions of language and practices of language education in the U.S. from 1869 - 1969.  Textbooks, linguistics, public literacy education policy and methods, government spending, mechanical representation, reproduction, and distribution technologies, writing machines, human sciences, definitions of the human, figurative language, definitions of language.  This book offers one explanation of the story of how machines were taught to read and write in the U.S.  I use the verb "taught" in a non-metaphorical sense, because the manner in which non-humans (mechanical and digital devices) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  The education of machines and the education of humans in the U.S. exists in a dialectical relationship.  The two are inextricably bound up with one another.  It is a complicated story about humans, government, and nation building.  Part of the nation's military budget was always being dedicated to literacy education.  It was used to teach machines to read and write instead of humans.  The reasons for this are diverse and probably involve the relations of social technologies with material technologies.  I have only scratched the surface in this project and by making the materials I have used for this project available to other researchers, it is my hope that more will be revealed.  This is the story of latin alphabetic systems of notation being made to control/merged with non-latin alphabetic systems of notation.  

We have undoubtedly reached a new point in human/machine relations when a comparative literature major can begin to think of the latin alphabet as but “one system of notation” among others.  

How do engineers think about language?  Do they?  I’ve asked myself this question for some time and my attempt to answer the question may be the best way to describe this book and its purposes.   

Applied Linguistics 
The Manual Humanities
Sociology of Literacy 

Ideas from The Prague School of functional linguistics have been adopted in NLP

Roman Jakobson writing about bits in his 1961 essay “Linguistics and Communication Theory” 

In this 1954 review of Willis’ book, the author proposes somewhat wistfully that the “inorganic world of mechanics” and the “immaterial realm of thought” might be brought together via a shared theoretical approach such as entropy and thermodynamics “Engineering.Language.Navy.Research.Lab.Review.Jackson.Communication.Theory.1953.1 

ChatGPT is one version of a mind, not a mind per se.  It is the “literate mind” that is discussed and described in the abstract and that has now been built thanks to a great deal of money and education.  

Project Overview

This project documents a genealogy of the relationships between human and non-human language and literacy education--and their relationships--from 1869 - 1969.   information and communications technologies defined broadly [as including technologies of print reproduction and social technologies (see Bazerman) and their relationships with definitions of language and practices of language education in the U.S. from 1869 - 1969.  Textbooks, linguistics, public literacy education policy and methods, government spending, mechanical representation, reproduction, and distribution technologies, writing machines, human sciences, definitions of the human, figurative language, definitions of language.  This is one version of the story of how machines were taught to read and write in the U.S.  I use the verb "taught" in a non-metaphorical sense, because the manner in which non-humans (mechanical and digital devices) have been endowed with the ability to manipulate symbols in a manner that emulates human communication and sign making processes has involved a great deal of education.  The education of machines and the education of humans in the U.S. exists in a dialectical relationship.  The two are inextricably bound up with one another.  It is a complicated story about humans, government, and nation building.  Part of the nation's military budget was always being dedicated to literacy education.  It was used to teach machines to read and write instead of humans.  The reasons for this are diverse and probably involve the relations of social technologies with material technologies.  I have only scratched the surface in this project and by making the materials I have used for this project available to other researchers, it is my hope that more will be revealed.  This is the story of latin alphabetic systems of notation being made to control/merged with non-latin alphabetic systems of notation. 

ChatGPT is the latest instantiation of a complex of non-latin alphabetic systems of notation controlling the production and manipulation of latin alphabetic tokens, which humans usually refer to as words.  

medium = thing

mode (is this term always already a machine term?)

sense

cognition

The model used to teach digital electronic calculating machines to write was the current-traditional model of SWE education.  


Introduction

[What if] How might the reclining caret in the Unix command line can be in some way be connected to a mark signifying the emission of breath accompanying the formation of a spoken word?  What might that tell us about some of the many roles and functions of writing in the pasts, presents, and futures of computing machines?  While this may sound like an intriguing if somewhat implausible hypothesis, I hope to show that this connection between human and machine communication can be documented.  The story I would plan to tell provides one narrative connecting these two apparently disparate realms of communication (the human and machine) and in the process also traces connections amongst the spoken and the written, and the manual, mechanical, and digital, in ways that may offer some insights into the relationships amongst old and new technologies of inscription and representation and the social and material technological systems supporting them.  

First, there is the questions of what issues need to be addressed in establishing a framework between human inscription and representation practices and mechanical inscription and representation "practices."  


Although the book is busy enough with the histories of verbal signification and computation to not have much time for a consideration of the genealogies of “Artificial Intelligence,” it is implicitly connected to those technologies and narratives through its concern with alphabetic and machine literacies.  As Stephanie Dick has written in her work on the histories of Artificial Intelligence, “making up minds,” is, from a technical and cultural perspective, very much the concern of current media representations and technical implementations of what is now referred to generally as “AI.”   

	Introduction: Hermeneutics and Semiology: Exploring Relationships Between What and How Stories About Reading and Writing Are Told


Was AGB Raised as a Telephone? Alexander Melville’s Visible Speech as an Operational System of Notation and Educational Standard:


Latin for the Masses: Universal Languages and Markov’s Characters, or Logic and Statistics as Reading and Writing Instruments:


Telemental Models of Communication: Hartley’s Equation and Saussure’s Wires:


Before the Byte, There Was the Word: Von Neumann’s Word Choice:


Hopper’s Syllabus and Tukey’s Neologism
>, or How Processed Do You Like Your Humans: Thompson and Ritchie’s “Shorthand” for the Unix Operating System

	Epilogue: Eliza Doolittle's Ghosts

There is a lot that is not in this book that it is my hope that other scholars and historians will further investigate these themes and topics using the digital archive and bibliography accompanying this project.  

This project is also imbricated with my own literacy narrative, which began in the American midwest in 1968, was accredited first at Stanford University in 1990 when I graduated as a Comparative Literature major, developed in an applied professional capacity in Silicon Valley after my graduation from college, and broadly expanded by my work as a professional writer in New York City and my research and teaching with students at The City University of New York.  I do not hesitate these days to say that I have made my living as a professional in high tech, first in the private sector and later in the public sector.  High tech has become a part of every industry in the last thirty years, so this should not come as any great surprise.  However, what I did not understand until becoming more involved with the histories of computings was how much my educational training in philosophy, linguistics, writing, and textual studies had to do with engineering and its various disciplines.    

 
The problems this project seeks to address include:

1/ How digital electronic networked computational devices learned how to read and write, the discreet stages of this process, and the relationships between the technical engineering of material artifacts and of socio-cultural systems at each stage. 

2/ Why and how Standard Written English as a dialect becomes embedded in digital electronic networked computational devices and the socio-cultural and educational import of this. 

3//How rhetoric functions in technical documents to negotiate the boundaries between human and machine instrumentality and to promote the interests of some parties and interests over others.   

To address these issues, several popular and disciplinary myths need to be addressed: 

1/ That digital electronic computing systems have always been symbolic systems that “educated” themselves through automated and systematic ingenuity.
  
2/ That humanist analyses and critiques of human languages and its socio-cultural functions are of no import to the technical or applied applications of human and machine languages.   

3/ That the simultaneous deployment of  technical and instrumental uses of verbal language is not an event of some significance in the histories of computing and the histories of education in the U.S.. 

4/ That the expenditures on the higher literacy education of machines has been both inevitable and had anything but beneficial effects on the processes and purposes of human literacy education.   

* What previously unknown or unfortunately neglected story are you planning to tell?
How machines learned to read and write in the U.S. from 1869 - 1969 and how this “literacy narrative” is interwoven with the socio-economic, medial, technical, cultural, political, and educational currents of the same period.  Comparing and contrasting the rise of mechanical literacy and human literacy.  Inverse relationship.  

This book is about systems of notation.

* How is this book different from all other books?

It defines its difference from other books as provisional and invites scholarly collaboration in the discussion and research.  It presents its argument as one interpretation of a set of documentary evidence published in a shared digital archive for other scholars to use and expand upon.

It provides a rational for the field of the Manual/Applied Humanities/Critical Bibliography and Textual Studies by considering a history of instrumental inscriptive practices as including both social technologies (public literacy education, government funded initiatives related to defense, commerce, education, and administration, definitions and implementations of language in a range of higher educational disciplines) and material technologies (shorthand “alphabets,” printing presses, telegraphs, telephones, typewriters, teletypewriters, adding machines, cash registers, vacuum tubes, machine languages, keyboards, memory units, semiconductor chips, software systems and applications)  and traces the meeting of these two technological domains at specific moments in time based on evidence from discursive artifacts in government and corporate archives and publications.

It is written for a general audience.  It synthesizes the arguments of forty years of socio-cultural studies on computation and its relationships with literacy education practices and policies for a general audience.  It is an interdisciplinary documentary project aimed at synthesizing existing historical and cultural work on the histories of computation.  

It is relevant to contemporary issues and events. It suggests that computational technologies and their relationships with the socio-cultural, environmental, educational, and economic interests of the public need to be managed in the interests of something other than the logic of technics (Winner) as it has come to be defined in the early twenty-first century, i.e., efficiency and cost savings, to protect the well being of biological organisms, including humans.    

It is collaborative and provisional. 

Import

Audience


* Why does that matter? To whom?
Possible audiences are as variable as publishers. Consider:
* Is your book for specialists in your field?  It is for scholars working in the interdisciplinary fields of media archaeology, the manual humanities, software studies, science and technology studies
* Does your book focus on a particular area within a larger field?  
* Is it a book that students might use, and if so, students at what level?  It would be useful to graduate students in education, writing studies, STS.
* Is it a “trade” book? That is, one intended for general readers, those without specialized knowledge in your area?
Whatever your answer, consider carefully the kind of approach, terminology, level of explanation, and scholarly apparatus that your book will need to make it most compelling for your ideal reader.
Successful proposals usually include:
* A narrative description of the proposed book’s themes, arguments, goals, place in the literature, and expected audience. State your argument concisely and clearly.

How, when, and why did digital electronic networked computational devices become symbolic processing machines and how, when, and why do assumptions about and practices related to symbolic “processing” by humans inform the “literacy narratives” of these mechanical and electronic calculators and communications devices?  There are  several complications in posing this question in this form.  The first set of complications arise with the determined persistence with which  the myth that electronic computational devices sometime around 1950 simply arose as symbol processing machines (Dreyfus and Dreyfus 1988).  This is an ahistorical depiction of the emergence of these machines that has been reinforced by media networks and corporate interests.  These devices and the social and material technologies embedded in them have histories that have been well documented by scholars in a range of fields.  Yet, the dominant media and commercial narratives of symbolic processing and its applications, what is now generally categorized under the name of artificial intelligence, is ahistorical.   

The second set of complications arise in relation to the breadth and depth of the disciplinary specialization and technical expertise required to respond to a question involving the histories of so many different disciplines: computational devices, mathematics, psychology, brain science, linguistics, and engineering.  Yet, within engineering, applied versions of each of these disciplines are used in research and development of products and technologies and in 1986, this interdisciplinary field was given a name and an institutional role at Stanford University with the creation of the Symbolic Systems program.  While it is not possible to publish a scholarly book that encompasses the histories of the disciplines of linguistics, psychology, cognitive science, neurosurgery, and public education, applied versions of each of these disciplines are borrowed and freely adapted in the development of products and new technologies related to human language processing.  While there is an interdisciplinary field called “Symbolic Systems,” its history has been written in any number of disciplines making up the field.  This project suggests that this field of Symbolic Systems requires a history that includes a consideration of the material and the social technologies of signification and symbol making via archival textual evidence.      
The histories of public literacy education and the histories of machine “literacy” in the United States share an identical timeline.  Yet the two are rarely/surprisingly brought into dialogue with one another in relation to their shared concern with social and material technologies of inscription, or what is sometimes referred to as “writing.”  The term writing is problematic because it signifies so many different things depending on the context in which it is deployed.  It can signify making a marks on a page, the graphic depiction of speech, _____, or ______.   This term “writing,” whether being used as a synonym or synechdoche for literacy is one unique both in its vagueness/generality and scope/breadth.  Particularly in a socio-educational context this term “writing” as it relates to advanced literacy in Standard Written English is integral to a range of educational, social, cultural, and commercial operations and functions.  Instrumental uses of social and material technologies of inscription.  
                     
Having dedicated most of my education and professional life to understanding how humans read and write and the import of both acts, I also, because of the economic realities of my life and job market, spent ten years of my professional life becoming conversant in how electronic digital computers operate.  The interconnections and relationships between the literacy practices of both have been addressed by scholars in the social sciences and STS in different disciplines but never from the interdisciplinary perspective of instrumental literacy and signification functions.  
The scholarship on the roles of technologies in K-12 and higher ed writing instruction are distinct.  Until the early 2000s, there was surprisingly little scholarship in higher ed writing studies dedicated to discussions of writing technologies.  Christina Haas Writing Technology: Studies on the Materiality of Literacy (1996) is one exception.  Charles Bazerman’s Languages of Edison’s Light (1999) may be another though this book is usually categorized as being part of the history of science and technology studies rather than as part of the history of composition.  


Lorraine Daston: a history of inscriptive practices and technologies in Nation Building: Writing Studies, Literacy Education, Media Archaeology.  The Manual Humanities, which prioritizes the needs and specificities of human cognitive processes, human labor practices, and humans as a species in a highly delicate and interconnected global physical, social, biological, and geological environment.              
* A comparison of the proposed book to other books now available that are intended for the audience you seek. (If you are writing a specialized monograph, it is not especially illuminating to compare it to a popularized treatment of the same subject.)
David Golumbia
Matthew Fuller
Wendy Chung
Anne Pasek, et al
Cubitt, Sean. 2017. Finite Media: Environmental Implications of Digital Technologies. Durham, N.C.: Duke University Press.
* A summary of your own professional experience, past publications, and relevant research, aimed at explaining why you are the right author for the book you intend to write.
My own literacy, educational, and professional narrative exists in a metonymic relationship with the story told in this book. Remediation, disciplinary agonism, the roles of capital in higher education is part of what makes me uniquely qualified to write this book.  [Few people other than me could write this book because the story is in part a refraction of my own literacy and educational narrative.]  Born in 1968, I was raised and educated in Ann Arbor, Michigan, where I attended public school.  First, Burns Park Elementary School, which was one of the first public elementary schools built by the newly formed Department of Education in the U.S. Later, Tappan Middle School, named after _____, and finally Pioneer High School. 


Even in the early 1970s cultures of print were already facing off with new digital electronic technologies, one material trace of which was the x by y punch cards inside the front cover of every book sold at the local university bookstore, Borders.  At that time, one of the two Borders brothers was enrolled at the University of Michigan and had access to the mainframe computer.  He used the computing time granted to him to write a program for the inventory management of the bookstore.  Each night, ____ would collect the data cards from that day’s sales and update the store’s inventory.  I have only third hand information about the specific programs used, but I know that this early (1972) management and integration of sales and inventory systems were in part what led to the success of Borders Bookstore both as an independent business and later as a chain that was acquired by KMart.
I graduated from high school in 1986 and attended Stanford University as an undergraduate on a full scholarship, majoring in Comparative Literature.  Why I did not study computer science is, in the 21st century, hard for me to comprehend.   However at the time, this decision had to do with how the subject was taught –and largely still is taught– that I was categorized as a “verbal,” not a “math” person, and that the humanities, particularly textual studies, still appeared to be highly relevant to society and culture.  

Having been provisionally hired by the first management consulting firm in Silicon Valley in 1992 after I took a leave of absence from a Ph.D. Program in Comparative Literature at Yale University and was unable to find a job in print publishing in the San Francisco Bay Area, I spent the next ten years researching new technologies, primarily database software, in an office park shared by Sun Microsystems.  At that time, I never once considered learning Unix or SqlServer even though I discussed that operating system and database scripting language daily with the IT managers whom I interviewed and the marketing, communications, and strategic development officers at the then F1000 technology companies who were our clients.  I was privy, in 1995, to early discussions about the launch of Adobe’s Portable Document Format standard, which I would only come to understand in the 2020s as the corporate standardization of a publicly available file standard.  My colleagues at the management consulting firm, most of whom had attended Yale rather than Stanford, and who therefore knew what it actually meant to be a “Comparative Literature major” were amused, astonished, and pleased at how well a degree in philosophy, linguistics, and textual studies had prepared me to work as a research analyst in the high tech industry.    

In 1995, I relocated to Brooklyn, New York, continued to work remotely for the management consulting firm in Silicon Valley, and enrolled in a graduate creative writing program at the City College of New York.  It was by completing this program that I first became an adjunct writing instructor at the City University of New York, where I would complete my Ph.D. in English/Composition and Rhetoric in 2017.  I first enrolled at CCNY because I aspired to have a career in creative writing and I chose to attend a degree program I could easily afford.  The tuition at CUNY has since 1995 quadrupled.  My experiences in public higher ed have been, both as a student and as an instructor, decidedly mixed.  I believe in the mission of public education and yet I was also aware that I was treated much better at a private college.      

My intellectual, creative, and professional lives have been shaped by the intersection of the medial, technical, and economic realities in which I was raised and in which I have lived as part of a specific generation of well educated humans in the United States.  I would like humans who do not currently work in Engineering  and the disciplines related to it to know, first, that human language, its use, and its management has had a lot to do with how and why decisions have been made to prioritize the interests of electronic computational devices and the systems supporting them over those of soci0-cultural and planetary conservation, and, second, that, regardless of their disciplinary background and level of education,  they are qualified and capable of engaging in conversations related to decisions about the priorities for public policy and public spending that will be made in the next ten to twenty years.          

* An annotated table of contents, with a brief description of the contents of each chapter.
* An estimate of the probable length of the book, the illustrations (if any) that you wish to include, the time it will take you to write it, and any possible complicating factors.

Please send all proposals by email to submissions_HUP@harvard.edu (Attention: Editorial). You should expect an answer within three to four weeks. If you choose to send your proposal by U.S. mail or courier please be advised that we are not responsible for lost packages and we do not return unsolicited manuscripts or proposals.

Writing Studies

Linguistics

Semiotics

STS/Computer History

STS/Inscriptive Practices: Donna Haraway, Lily Kay, Friedrich Kittler, Lisa Gitelman, Johanna Drucker, Stephanie Dick, Chung, what's his face, Charles Bazerman, Sean Cubitt, 

Media Studies and Archaeology

Bibliography

Composition and Rhetoric/Communication Studies

Digital Humanities
